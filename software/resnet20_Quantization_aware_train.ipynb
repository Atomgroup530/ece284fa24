{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec3ca981",
   "metadata": {},
   "source": [
    "# Resnet20 Quant_aware_training with 4 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "joYgZCpmteq0",
   "metadata": {
    "id": "joYgZCpmteq0"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def weight_quantization(b):\n",
    "\n",
    "    def uniform_quant(x, b):\n",
    "        xdiv = x.mul((2 ** b - 1))\n",
    "        xhard = xdiv.round().div(2 ** b - 1)\n",
    "        #print('uniform quant bit: ', b)\n",
    "        return xhard\n",
    "\n",
    "    class _pq(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, alpha):\n",
    "            input.div_(alpha)                          # weights are first divided by alpha\n",
    "            input_c = input.clamp(min=-1, max=1)       # then clipped to [-1,1]\n",
    "            sign = input_c.sign()\n",
    "            input_abs = input_c.abs()\n",
    "            input_q = uniform_quant(input_abs, b).mul(sign)\n",
    "            ctx.save_for_backward(input, input_q)\n",
    "            input_q = input_q.mul(alpha)               # rescale to the original range\n",
    "            return input_q\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            grad_input = grad_output.clone()             # grad for weights will not be clipped\n",
    "            input, input_q = ctx.saved_tensors\n",
    "            i = (input.abs()>1.).float()     # >1 means clipped. # output matrix is a form of [True, False, True, ...]\n",
    "            sign = input.sign()              # output matrix is a form of [+1, -1, -1, +1, ...]\n",
    "            #grad_alpha = (grad_output*(sign*i + (input_q-input)*(1-i))).sum()\n",
    "            grad_alpha = (grad_output*(sign*i + (0.0)*(1-i))).sum()\n",
    "            # above line, if i = True,  and sign = +1, \"grad_alpha = grad_output * 1\"\n",
    "            #             if i = False, \"grad_alpha = grad_output * (input_q-input)\"\n",
    "            grad_input = grad_input*(1-i)\n",
    "            return grad_input, grad_alpha\n",
    "\n",
    "    return _pq().apply\n",
    "\n",
    "\n",
    "class weight_quantize_fn(nn.Module):\n",
    "    def __init__(self, w_bit):\n",
    "        super(weight_quantize_fn, self).__init__()\n",
    "        self.w_bit = w_bit-1\n",
    "        self.weight_q = weight_quantization(b=self.w_bit)\n",
    "        self.register_parameter('wgt_alpha', Parameter(torch.tensor(3.0)))\n",
    "\n",
    "    def forward(self, weight):\n",
    "        mean = weight.data.mean()\n",
    "        std = weight.data.std()\n",
    "        weight = weight.add(-mean).div(std)      # weights normalization\n",
    "        weight_q = self.weight_q(weight, self.wgt_alpha)\n",
    "\n",
    "        return weight_q\n",
    "\n",
    "\n",
    "def act_quantization(b):\n",
    "\n",
    "    def uniform_quant(x, b=4):\n",
    "        xdiv = x.mul(2 ** b - 1)\n",
    "        xhard = xdiv.round().div(2 ** b - 1)\n",
    "        return xhard\n",
    "\n",
    "    class _uq(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, alpha):\n",
    "            input=input.div(alpha)\n",
    "            input_c = input.clamp(max=1)  # Mingu edited for Alexnet\n",
    "            input_q = uniform_quant(input_c, b)\n",
    "            ctx.save_for_backward(input, input_q)\n",
    "            input_q = input_q.mul(alpha)\n",
    "            return input_q\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            grad_input = grad_output.clone()\n",
    "            input, input_q = ctx.saved_tensors\n",
    "            i = (input > 1.).float()\n",
    "            #grad_alpha = (grad_output * (i + (input_q - input) * (1 - i))).sum()\n",
    "            grad_alpha = (grad_output * (i + (0.0)*(1-i))).sum()\n",
    "            grad_input = grad_input*(1-i)\n",
    "            return grad_input, grad_alpha\n",
    "\n",
    "    return _uq().apply\n",
    "\n",
    "\n",
    "class QuantConv2d(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=False):\n",
    "        super(QuantConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups,\n",
    "                                          bias)\n",
    "        self.layer_type = 'QuantConv2d'\n",
    "        self.bit = 4\n",
    "        self.weight_quant = weight_quantize_fn(w_bit=self.bit)\n",
    "        self.act_alq = act_quantization(self.bit)\n",
    "        self.act_alpha = torch.nn.Parameter(torch.tensor(8.0))\n",
    "        self.weight_q  = torch.nn.Parameter(torch.zeros([out_channels, in_channels, kernel_size, kernel_size]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight_q = self.weight_quant(self.weight)\n",
    "        #self.register_parameter('weight_q', Parameter(weight_q))  # Mingu added\n",
    "        self.weight_q = torch.nn.Parameter(weight_q)  # Store weight_q during the training\n",
    "        x = self.act_alq(x, self.act_alpha)\n",
    "        return F.conv2d(x, weight_q, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def show_params(self):\n",
    "        wgt_alpha = round(self.weight_quant.wgt_alpha.data.item(), 3)\n",
    "        act_alpha = round(self.act_alpha.data.item(), 3)\n",
    "        print('clipping threshold weight alpha: {:2f}, activation alpha: {:2f}'.format(wgt_alpha, act_alpha))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "OifnJ9ndtlfc",
   "metadata": {
    "id": "OifnJ9ndtlfc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "resnet for cifar in pytorch\n",
    "Reference:\n",
    "[1] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.\n",
    "[2] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, 2016.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \" 3x3 convolution with padding \"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "def Quantconv3x3(in_planes, out_planes, stride=1):\n",
    "    \" 3x3 quantized convolution with padding \"\n",
    "    return QuantConv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion=1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, float=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if float:\n",
    "            self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "            self.conv2 = conv3x3(planes, planes)\n",
    "        else:\n",
    "            self.conv1 = Quantconv3x3(inplanes, planes, stride)\n",
    "            self.conv2 = Quantconv3x3(planes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion=4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes*4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes*4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet_Cifar(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=10, float=False):\n",
    "        super(ResNet_Cifar, self).__init__()\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0], float=float)\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2, float=float)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2, float=float)\n",
    "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
    "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, float=False):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                QuantConv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False)\n",
    "                if float is False else nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1,\n",
    "                                                 stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, float=float))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, float=float))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def show_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, QuantConv2d):\n",
    "                m.show_params()\n",
    "\n",
    "\n",
    "def resnet20_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [3, 3, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet32_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [5, 5, 5], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet44_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [7, 7, 7], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet56_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [9, 9, 9], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet110_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [18, 18, 18], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet1202_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [200, 200, 200], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet164_quant(**kwargs):\n",
    "    model = ResNet_Cifar(Bottleneck, [18, 18, 18], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet1001_quant(**kwargs):\n",
    "    model = ResNet_Cifar(Bottleneck, [111, 111, 111], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "    # net = resnet20_cifar(float=True)\n",
    "    # y = net(torch.randn(1, 3, 64, 64))\n",
    "    # print(net)\n",
    "    # print(y.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "radical-fifty",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "radical-fifty",
    "outputId": "87c82510-9a95-49c1-db46-ed9f6f74f6cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "ResNet_Cifar(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "model_name = \"resnet20_quant_4bit\"\n",
    "model = resnet20_quant()\n",
    "\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [10, 15]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "junior-reminder",
   "metadata": {
    "id": "junior-reminder"
   },
   "outputs": [],
   "source": [
    "# # This cell won't be given, but students will complete the training\n",
    "\n",
    "# lr = 4e-2\n",
    "# weight_decay = 1e-4\n",
    "# epochs = 50\n",
    "# best_prec = 0\n",
    "\n",
    "# #model = nn.DataParallel(model).cuda()\n",
    "# model.cuda()\n",
    "# criterion = nn.CrossEntropyLoss().cuda()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "# #cudnn.benchmark = True\n",
    "\n",
    "# if not os.path.exists('result'):\n",
    "#     os.makedirs('result')\n",
    "# fdir = 'result/'+str(model_name)\n",
    "# if not os.path.exists(fdir):\n",
    "#     os.makedirs(fdir)\n",
    "\n",
    "\n",
    "# for epoch in range(0, epochs):\n",
    "#     adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "#     train(trainloader, model, criterion, optimizer, epoch)\n",
    "\n",
    "#     # evaluate on test set\n",
    "#     print(\"Validation starts\")\n",
    "#     prec = validate(testloader, model, criterion)\n",
    "\n",
    "#     # remember best precision and save checkpoint\n",
    "#     is_best = prec > best_prec\n",
    "#     best_prec = max(prec,best_prec)\n",
    "#     print('best acc: {:1f}'.format(best_prec))\n",
    "#     save_checkpoint({\n",
    "#         'epoch': epoch + 1,\n",
    "#         'state_dict': model.state_dict(),\n",
    "#         'best_prec': best_prec,\n",
    "#         'optimizer': optimizer.state_dict(),\n",
    "#     }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decreased-harris",
   "metadata": {
    "id": "decreased-harris"
   },
   "outputs": [],
   "source": [
    "# HW\n",
    "\n",
    "#  1. Train with 4 bits for both weight and activation to achieve >90% accuracy\n",
    "#  2. Find x_int and w_int for the 2nd convolution layer\n",
    "#  3. Check the recovered psum has similar value to the un-quantized original psum\n",
    "#     (such as example 1 in W3S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "entertaining-queensland",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "entertaining-queensland",
    "outputId": "fb336ebf-4ce2-42b9-f2ea-f82cd7e22833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 8679/10000 (87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/resnet20_quant_4bit/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "QAvDEJYD6Jcs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QAvDEJYD6Jcs",
    "outputId": "7124ff18-ab88-4434-e86b-43b4a92e50e7"
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceramic-nigeria",
   "metadata": {
    "id": "ceramic-nigeria"
   },
   "outputs": [],
   "source": [
    "#send an input and grap the value by using prehook like HW3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spoken-worst",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spoken-worst",
    "outputId": "4189fef0-5bf3-4bbd-e53a-a9949aed53fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.,  0., -0.],\n",
      "          [-0.,  1.,  0.],\n",
      "          [ 1.,  1., -1.]],\n",
      "\n",
      "         [[ 6.,  3.,  2.],\n",
      "          [ 3.,  0., -0.],\n",
      "          [-7., -3., -6.]],\n",
      "\n",
      "         [[ 2.,  2., -2.],\n",
      "          [ 5.,  1., -2.],\n",
      "          [ 2.,  2.,  1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.,  1., -2.],\n",
      "          [ 2.,  1., -5.],\n",
      "          [ 3.,  1., -5.]],\n",
      "\n",
      "         [[-1.,  1.,  2.],\n",
      "          [-2., -2., -1.],\n",
      "          [ 6.,  0., -1.]],\n",
      "\n",
      "         [[ 0.,  1.,  1.],\n",
      "          [ 1.,  2.,  3.],\n",
      "          [-0., -2.,  0.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.],\n",
      "          [ 1.,  0., -1.],\n",
      "          [-0., -0., -0.]],\n",
      "\n",
      "         [[ 2.,  2.,  2.],\n",
      "          [-1., -2., -6.],\n",
      "          [ 1.,  1., -3.]],\n",
      "\n",
      "         [[-3., -3., -2.],\n",
      "          [-2., -2.,  1.],\n",
      "          [ 1.,  5.,  7.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.,  2., -0.],\n",
      "          [ 3.,  1., -0.],\n",
      "          [ 1., -3., -5.]],\n",
      "\n",
      "         [[-7., -5., -5.],\n",
      "          [ 6.,  5.,  2.],\n",
      "          [-0.,  0.,  0.]],\n",
      "\n",
      "         [[ 1.,  0.,  1.],\n",
      "          [ 1.,  0., -0.],\n",
      "          [-1., -2., -3.]]],\n",
      "\n",
      "\n",
      "        [[[-1., -1., -1.],\n",
      "          [-1., -1.,  0.],\n",
      "          [-1.,  1.,  0.]],\n",
      "\n",
      "         [[ 2., -2., -2.],\n",
      "          [ 0., -1., -5.],\n",
      "          [-1.,  1.,  1.]],\n",
      "\n",
      "         [[ 1., -0.,  1.],\n",
      "          [-0.,  1.,  3.],\n",
      "          [-3.,  1.,  1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1., -0.,  0.],\n",
      "          [ 0., -1., -1.],\n",
      "          [ 5.,  2., -1.]],\n",
      "\n",
      "         [[-1., -1.,  2.],\n",
      "          [-2.,  0.,  3.],\n",
      "          [ 0., -3., -2.]],\n",
      "\n",
      "         [[ 0.,  1., -2.],\n",
      "          [ 3.,  2., -1.],\n",
      "          [-0.,  1., -0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-5., -6., -6.],\n",
      "          [-1., -1., -2.],\n",
      "          [ 1.,  0.,  0.]],\n",
      "\n",
      "         [[-1., -1., -1.],\n",
      "          [-0., -1., -1.],\n",
      "          [-2.,  0.,  0.]],\n",
      "\n",
      "         [[ 1., -0., -1.],\n",
      "          [ 5.,  3.,  1.],\n",
      "          [ 3.,  2., -1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2., -3., -3.],\n",
      "          [-0., -1., -2.],\n",
      "          [ 5.,  1.,  1.]],\n",
      "\n",
      "         [[-1., -1.,  0.],\n",
      "          [-1.,  1.,  2.],\n",
      "          [-1., -2.,  1.]],\n",
      "\n",
      "         [[ 2.,  1., -1.],\n",
      "          [ 2.,  1.,  1.],\n",
      "          [-1.,  2.,  2.]]],\n",
      "\n",
      "\n",
      "        [[[-0., -2., -1.],\n",
      "          [ 2., -0.,  0.],\n",
      "          [ 1.,  0., -0.]],\n",
      "\n",
      "         [[-3., -2.,  1.],\n",
      "          [-6.,  0.,  5.],\n",
      "          [-7.,  1.,  5.]],\n",
      "\n",
      "         [[ 8., -5., -8.],\n",
      "          [ 8.,  1., -3.],\n",
      "          [ 2.,  5., -1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.,  1.,  2.],\n",
      "          [-0., -2., -3.],\n",
      "          [-1., -1., -1.]],\n",
      "\n",
      "         [[ 0., -1.,  1.],\n",
      "          [ 0., -0., -1.],\n",
      "          [-2., -3., -1.]],\n",
      "\n",
      "         [[ 1.,  2.,  1.],\n",
      "          [ 1.,  1.,  1.],\n",
      "          [-3.,  1.,  2.]]],\n",
      "\n",
      "\n",
      "        [[[ 2., -0.,  0.],\n",
      "          [-0., -2., -2.],\n",
      "          [ 0.,  1., -2.]],\n",
      "\n",
      "         [[ 2., -2., -6.],\n",
      "          [ 1., -1., -2.],\n",
      "          [ 5.,  0., -1.]],\n",
      "\n",
      "         [[ 0.,  0., -1.],\n",
      "          [-1.,  2., -1.],\n",
      "          [-5.,  1.,  3.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6., -1., -6.],\n",
      "          [ 7.,  0., -5.],\n",
      "          [ 5.,  1., -2.]],\n",
      "\n",
      "         [[-2.,  0.,  1.],\n",
      "          [-5., -2.,  1.],\n",
      "          [-2.,  2.,  3.]],\n",
      "\n",
      "         [[ 0., -0.,  2.],\n",
      "          [-1.,  2.,  0.],\n",
      "          [-1., -1.,  3.]]]], device='cuda:0', grad_fn=<RoundBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w_bit = 4\n",
    "weight_q = model.layer1[0].conv1.weight_q # quantized value is stored during the training\n",
    "w_alpha = model.layer1[0].conv1.weight_quant.wgt_alpha   # alpha is defined in your model already. bring it out here\n",
    "w_delta = w_alpha / (2 ** (w_bit - 1))   # delta can be calculated by using alpha and w_bit\n",
    "weight_int = (weight_q / w_delta).round() # w_int can be calculated by weight_q and w_delta\n",
    "print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "-lYYXMoe3bgN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lYYXMoe3bgN",
    "outputId": "3597883f-86e2-48cc-bcd3-0f08ba8cc0ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st convolution's input size: torch.Size([128, 3, 32, 32])\n",
      "2nd convolution's input size: torch.Size([128, 16, 32, 32])\n",
      "3rd convolution's input size: torch.Size([128, 16, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "\n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "#         print(\"prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)       ## Input for the module will be grapped\n",
    "####################################################\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.cuda()\n",
    "out = model(images)\n",
    "print(\"1st convolution's input size:\", save_output.outputs[0][0].size())\n",
    "print(\"2nd convolution's input size:\", save_output.outputs[1][0].size())\n",
    "print(\"3rd convolution's input size:\", save_output.outputs[2][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "interior-oxygen",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "interior-oxygen",
    "outputId": "f1fee443-04fa-42ec-a939-0a9a1ae2cb7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[2., 2., 1.,  ..., 2., 2., 0.],\n",
      "          [3., 2., 0.,  ..., 0., 0., 0.],\n",
      "          [3., 2., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [3., 2., 0.,  ..., 0., 0., 0.],\n",
      "          [3., 2., 0.,  ..., 0., 0., 0.],\n",
      "          [3., 3., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "          [2., 2., 5.,  ..., 5., 7., 7.],\n",
      "          [2., 2., 3.,  ..., 0., 2., 1.],\n",
      "          ...,\n",
      "          [2., 2., 3.,  ..., 2., 2., 1.],\n",
      "          [2., 2., 3.,  ..., 2., 2., 1.],\n",
      "          [4., 4., 4.,  ..., 2., 1., 0.]],\n",
      "\n",
      "         [[1., 1., 0.,  ..., 2., 1., 4.],\n",
      "          [2., 3., 0.,  ..., 2., 2., 5.],\n",
      "          [2., 3., 0.,  ..., 1., 1., 3.],\n",
      "          ...,\n",
      "          [2., 3., 0.,  ..., 3., 1., 3.],\n",
      "          [2., 3., 0.,  ..., 3., 1., 3.],\n",
      "          [3., 4., 3.,  ..., 2., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[3., 4., 2.,  ..., 1., 0., 0.],\n",
      "          [2., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [2., 1., 0.,  ..., 1., 0., 1.],\n",
      "          ...,\n",
      "          [2., 1., 0.,  ..., 2., 1., 2.],\n",
      "          [2., 1., 0.,  ..., 2., 1., 2.],\n",
      "          [0., 0., 0.,  ..., 2., 2., 3.]],\n",
      "\n",
      "         [[6., 7., 6.,  ..., 7., 6., 5.],\n",
      "          [1., 2., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 2., 1.,  ..., 2., 2., 2.],\n",
      "          ...,\n",
      "          [1., 2., 1.,  ..., 1., 1., 2.],\n",
      "          [1., 2., 1.,  ..., 1., 1., 2.],\n",
      "          [1., 2., 2.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[4., 2., 0.,  ..., 1., 0., 0.],\n",
      "          [4., 2., 0.,  ..., 0., 0., 2.],\n",
      "          [4., 2., 0.,  ..., 0., 0., 2.],\n",
      "          ...,\n",
      "          [4., 2., 0.,  ..., 1., 0., 4.],\n",
      "          [4., 2., 0.,  ..., 1., 0., 4.],\n",
      "          [3., 1., 0.,  ..., 1., 0., 3.]]],\n",
      "\n",
      "\n",
      "        [[[2., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [3., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [3., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [3., 2., 2.,  ..., 2., 2., 0.],\n",
      "          [3., 2., 2.,  ..., 2., 2., 0.],\n",
      "          [3., 3., 3.,  ..., 3., 3., 1.]],\n",
      "\n",
      "         [[0., 0., 1.,  ..., 3., 3., 2.],\n",
      "          [2., 3., 3.,  ..., 3., 3., 3.],\n",
      "          [2., 3., 3.,  ..., 2., 3., 3.],\n",
      "          ...,\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [4., 4., 4.,  ..., 4., 4., 4.]],\n",
      "\n",
      "         [[1., 1., 4.,  ..., 5., 4., 3.],\n",
      "          [2., 1., 3.,  ..., 3., 3., 3.],\n",
      "          [2., 1., 3.,  ..., 3., 3., 3.],\n",
      "          ...,\n",
      "          [2., 3., 3.,  ..., 3., 3., 1.],\n",
      "          [2., 3., 3.,  ..., 3., 3., 1.],\n",
      "          [3., 4., 4.,  ..., 4., 4., 3.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[3., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 0., 1.,  ..., 2., 2., 2.],\n",
      "          [2., 0., 1.,  ..., 3., 2., 3.],\n",
      "          ...,\n",
      "          [2., 1., 1.,  ..., 1., 1., 0.],\n",
      "          [2., 1., 1.,  ..., 1., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[6., 4., 2.,  ..., 0., 0., 1.],\n",
      "          [1., 1., 2.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 2.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [1., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [1., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [1., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "         [[4., 0., 0.,  ..., 2., 2., 3.],\n",
      "          [4., 0., 0.,  ..., 2., 3., 3.],\n",
      "          [4., 0., 0.,  ..., 2., 3., 3.],\n",
      "          ...,\n",
      "          [4., 2., 2.,  ..., 2., 2., 0.],\n",
      "          [4., 2., 2.,  ..., 2., 2., 0.],\n",
      "          [3., 1., 1.,  ..., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 1.,  ..., 2., 2., 0.],\n",
      "          [0., 0., 0.,  ..., 3., 2., 0.],\n",
      "          [0., 0., 0.,  ..., 2., 2., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 3., 2., 0.],\n",
      "          [0., 0., 1.,  ..., 3., 2., 0.],\n",
      "          [0., 0., 0.,  ..., 3., 3., 1.]],\n",
      "\n",
      "         [[1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 2., 3.,  ..., 1., 2., 2.],\n",
      "          [0., 0., 1.,  ..., 1., 2., 2.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 1., 2., 2.],\n",
      "          [0., 1., 2.,  ..., 1., 2., 2.],\n",
      "          [2., 3., 3.,  ..., 3., 4., 4.]],\n",
      "\n",
      "         [[1., 1., 2.,  ..., 1., 1., 0.],\n",
      "          [1., 0., 1.,  ..., 3., 3., 1.],\n",
      "          [1., 2., 1.,  ..., 3., 3., 1.],\n",
      "          ...,\n",
      "          [3., 3., 4.,  ..., 2., 3., 1.],\n",
      "          [4., 5., 5.,  ..., 2., 3., 1.],\n",
      "          [3., 3., 3.,  ..., 3., 4., 3.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0., 1., 1.,  ..., 3., 4., 2.],\n",
      "          [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "          ...,\n",
      "          [1., 1., 2.,  ..., 2., 1., 0.],\n",
      "          [1., 1., 1.,  ..., 2., 1., 0.],\n",
      "          [1., 1., 2.,  ..., 1., 0., 0.]],\n",
      "\n",
      "         [[2., 3., 3.,  ..., 5., 7., 4.],\n",
      "          [1., 0., 1.,  ..., 1., 2., 2.],\n",
      "          [3., 1., 0.,  ..., 1., 2., 2.],\n",
      "          ...,\n",
      "          [3., 2., 2.,  ..., 1., 2., 2.],\n",
      "          [2., 1., 1.,  ..., 1., 2., 2.],\n",
      "          [0., 0., 0.,  ..., 1., 2., 2.]],\n",
      "\n",
      "         [[2., 2., 2.,  ..., 4., 2., 0.],\n",
      "          [3., 2., 2.,  ..., 3., 2., 0.],\n",
      "          [3., 1., 1.,  ..., 3., 2., 0.],\n",
      "          ...,\n",
      "          [3., 1., 1.,  ..., 7., 2., 0.],\n",
      "          [2., 0., 1.,  ..., 7., 2., 0.],\n",
      "          [1., 1., 1.,  ..., 4., 1., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 2., 2., 0.],\n",
      "          [0., 0., 0.,  ..., 2., 2., 0.],\n",
      "          [0., 0., 0.,  ..., 2., 2., 0.],\n",
      "          ...,\n",
      "          [3., 2., 2.,  ..., 2., 2., 0.],\n",
      "          [3., 2., 2.,  ..., 2., 2., 0.],\n",
      "          [3., 3., 3.,  ..., 3., 3., 1.]],\n",
      "\n",
      "         [[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 2.,  ..., 2., 2., 2.],\n",
      "          [1., 1., 1.,  ..., 2., 2., 2.],\n",
      "          ...,\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [4., 4., 4.,  ..., 4., 4., 4.]],\n",
      "\n",
      "         [[2., 2., 2.,  ..., 1., 1., 0.],\n",
      "          [2., 3., 2.,  ..., 3., 3., 1.],\n",
      "          [2., 3., 2.,  ..., 3., 3., 1.],\n",
      "          ...,\n",
      "          [2., 3., 3.,  ..., 3., 3., 1.],\n",
      "          [2., 3., 3.,  ..., 3., 3., 1.],\n",
      "          [3., 4., 4.,  ..., 4., 4., 3.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2., 2., 1.,  ..., 4., 4., 2.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "          ...,\n",
      "          [2., 1., 1.,  ..., 1., 1., 0.],\n",
      "          [2., 1., 1.,  ..., 1., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[2., 3., 3.,  ..., 7., 7., 4.],\n",
      "          [1., 1., 0.,  ..., 2., 2., 2.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          ...,\n",
      "          [1., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [1., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [1., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "         [[2., 2., 1.,  ..., 2., 2., 0.],\n",
      "          [2., 2., 1.,  ..., 2., 2., 0.],\n",
      "          [2., 1., 1.,  ..., 2., 2., 0.],\n",
      "          ...,\n",
      "          [4., 2., 2.,  ..., 2., 2., 0.],\n",
      "          [4., 2., 2.,  ..., 2., 2., 0.],\n",
      "          [3., 1., 1.,  ..., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[2., 1., 1.,  ..., 2., 2., 0.],\n",
      "          [2., 2., 1.,  ..., 3., 2., 0.],\n",
      "          [2., 2., 1.,  ..., 3., 2., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 2., 2., 0.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 0.],\n",
      "          [3., 3., 3.,  ..., 3., 3., 1.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [2., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [3., 3., 3.,  ..., 2., 2., 2.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 2., 2., 2.],\n",
      "          [0., 0., 0.,  ..., 1., 2., 2.],\n",
      "          [4., 4., 4.,  ..., 4., 4., 4.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 0.],\n",
      "          [2., 3., 2.,  ..., 3., 3., 1.],\n",
      "          [3., 3., 2.,  ..., 3., 3., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 2., 3., 1.],\n",
      "          [0., 0., 1.,  ..., 3., 3., 1.],\n",
      "          [3., 4., 4.,  ..., 4., 4., 3.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[3., 3., 2.,  ..., 3., 4., 2.],\n",
      "          [1., 1., 0.,  ..., 2., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 2., 1., 0.],\n",
      "          ...,\n",
      "          [4., 6., 5.,  ..., 2., 1., 0.],\n",
      "          [3., 4., 4.,  ..., 2., 1., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[5., 6., 5.,  ..., 6., 7., 4.],\n",
      "          [1., 2., 2.,  ..., 2., 2., 2.],\n",
      "          [1., 2., 2.,  ..., 2., 2., 2.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 2., 2., 2.],\n",
      "          [7., 8., 6.,  ..., 3., 2., 2.],\n",
      "          [1., 2., 2.,  ..., 2., 2., 2.]],\n",
      "\n",
      "         [[4., 2., 1.,  ..., 4., 2., 0.],\n",
      "          [4., 1., 0.,  ..., 3., 2., 0.],\n",
      "          [3., 1., 0.,  ..., 3., 2., 0.],\n",
      "          ...,\n",
      "          [2., 3., 4.,  ..., 3., 2., 0.],\n",
      "          [4., 2., 2.,  ..., 2., 2., 0.],\n",
      "          [3., 1., 1.,  ..., 1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 2., 3.,  ..., 3., 3., 1.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 1., 2.,  ..., 2., 2., 2.],\n",
      "          [5., 8., 8.,  ..., 8., 8., 8.],\n",
      "          [3., 2., 2.,  ..., 0., 1., 1.],\n",
      "          ...,\n",
      "          [3., 1., 1.,  ..., 2., 2., 1.],\n",
      "          [3., 2., 2.,  ..., 2., 1., 1.],\n",
      "          [4., 1., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 3., 3.,  ..., 3., 3., 4.],\n",
      "          [0., 4., 4.,  ..., 3., 4., 5.],\n",
      "          [0., 2., 1.,  ..., 1., 1., 3.],\n",
      "          ...,\n",
      "          [0., 3., 2.,  ..., 2., 2., 3.],\n",
      "          [0., 3., 2.,  ..., 2., 2., 3.],\n",
      "          [2., 2., 1.,  ..., 0., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 1.,  ..., 2., 1., 2.],\n",
      "          ...,\n",
      "          [0., 1., 2.,  ..., 1., 1., 2.],\n",
      "          [0., 1., 1.,  ..., 1., 1., 2.],\n",
      "          [0., 1., 3.,  ..., 4., 4., 4.]],\n",
      "\n",
      "         [[5., 7., 7.,  ..., 7., 7., 5.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 2., 0.,  ..., 1., 0., 1.],\n",
      "          ...,\n",
      "          [0., 3., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 2., 0.,  ..., 0., 0., 1.],\n",
      "          [1., 2., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 1.,  ..., 1., 1., 0.],\n",
      "          [0., 0., 1.,  ..., 0., 1., 5.],\n",
      "          [0., 0., 2.,  ..., 1., 1., 5.],\n",
      "          ...,\n",
      "          [0., 0., 2.,  ..., 2., 2., 5.],\n",
      "          [0., 0., 2.,  ..., 2., 2., 5.],\n",
      "          [0., 0., 2.,  ..., 2., 2., 4.]]]], device='cuda:0',\n",
      "       grad_fn=<RoundBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_bit = 4\n",
    "x = save_output.outputs[1][0]  # input of the 2nd conv layer\n",
    "x_alpha  = model.layer1[0].conv1.act_alpha\n",
    "x_delta = x_alpha / (2 ** (x_bit - 1))\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
    "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
    "\n",
    "x_int = (x_q / x_delta).round()\n",
    "print(x_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ranging-porter",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ranging-porter",
    "outputId": "077066f2-6b23-4a6b-9e67-e94f6b992841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[   7.6632,   14.2695,  -22.1970,  ...,   38.0520,   42.8085,\n",
      "              7.3990],\n",
      "          [   5.0207,    5.8135,   -9.2487,  ...,    9.7772,  -10.5700,\n",
      "            -20.8757],\n",
      "          [   5.2850,    3.9637,  -16.1192,  ...,    0.7927,  -11.8912,\n",
      "             -9.7772],\n",
      "          ...,\n",
      "          [   5.2850,    3.4352,  -17.9690,  ...,    2.3782,    6.6062,\n",
      "              5.5492],\n",
      "          [   5.2850,    3.4352,  -17.9690,  ...,   -3.1710,    6.0777,\n",
      "              5.5492],\n",
      "          [   7.9275,    0.7927,  -28.8032,  ...,   -7.1347,    5.8135,\n",
      "              1.0570]],\n",
      "\n",
      "         [[ -44.1297,  -43.0727,  -87.4667,  ...,  -95.6584,  -88.5237,\n",
      "            -71.3475],\n",
      "          [   3.9637,   10.0415,   20.0830,  ...,   59.9847,   47.0365,\n",
      "             25.6322],\n",
      "          [   2.9067,    1.0570,   -5.8135,  ...,   -5.2850,  -13.4767,\n",
      "            -12.1555],\n",
      "          ...,\n",
      "          [   2.9067,    0.5285,   -6.6062,  ...,   -8.4560,   -6.6062,\n",
      "             -9.5130],\n",
      "          [   2.9067,    0.5285,   -6.6062,  ...,   -8.9845,   -6.6062,\n",
      "             -9.5130],\n",
      "          [   9.7772,    6.3420,   -7.3990,  ...,  -11.3627,   -9.7772,\n",
      "            -13.2125]],\n",
      "\n",
      "         [[ -33.8240, -103.3217, -172.2909,  ...,  -52.0572,  -25.1037,\n",
      "            -92.4874],\n",
      "          [ -35.1452,  -98.0367, -184.4464,  ...,  -46.7722,  -54.9640,\n",
      "           -129.7467],\n",
      "          [ -34.3525,  -93.5444, -187.6174,  ...,  -47.0365,  -74.5185,\n",
      "           -135.5602],\n",
      "          ...,\n",
      "          [ -34.3525,  -93.0159, -186.5604,  ...,  -32.7670,  -38.8447,\n",
      "            -88.2594],\n",
      "          [ -34.3525,  -93.0159, -186.5604,  ...,  -41.2230,  -38.0520,\n",
      "            -88.2594],\n",
      "          [ -36.9950,  -94.6014, -191.3169,  ...,  -52.8500,  -35.1452,\n",
      "            -88.7879]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  11.6270,   17.9690,  -36.7307,  ...,   -9.5130,    7.9275,\n",
      "              2.6425],\n",
      "          [  26.4250,   39.6375,  -10.8342,  ...,   20.3472,   24.0467,\n",
      "             12.9482],\n",
      "          [  29.0675,   39.6375,  -16.6477,  ...,   13.7410,   23.7825,\n",
      "             12.9482],\n",
      "          ...,\n",
      "          [  29.0675,   39.9017,  -17.7047,  ...,    6.3420,   16.6477,\n",
      "             18.4975],\n",
      "          [  29.0675,   39.9017,  -17.7047,  ...,    4.4922,   16.6477,\n",
      "             18.4975],\n",
      "          [  26.4250,   36.2022,   -8.7202,  ...,   -0.2642,   16.1192,\n",
      "             18.4975]],\n",
      "\n",
      "         [[  13.4767,   21.6685,  -49.6790,  ...,  -30.1245,    7.1347,\n",
      "             13.7410],\n",
      "          [   7.3990,    9.2487,  -42.0157,  ...,   -0.7927,   22.1970,\n",
      "            -14.0052],\n",
      "          [   8.1917,   12.1555,  -37.2592,  ...,   10.5700,   15.0622,\n",
      "            -14.5337],\n",
      "          ...,\n",
      "          [   8.1917,   11.8912,  -36.2022,  ...,    5.8135,    8.7202,\n",
      "             -1.0570],\n",
      "          [   8.1917,   11.8912,  -36.2022,  ...,    3.9637,    8.7202,\n",
      "             -1.0570],\n",
      "          [   3.1710,   19.5545,  -26.9535,  ...,   -4.7565,    5.0207,\n",
      "              0.5285]],\n",
      "\n",
      "         [[  10.8342,  -38.3162,   55.7567,  ...,   18.4975,   -3.1710,\n",
      "            -37.7877],\n",
      "          [   1.5855,  -17.9690,  -12.1555,  ...,  -13.4767,  -20.6115,\n",
      "             -2.3782],\n",
      "          [   2.1140,   -9.5130,   10.5700,  ...,   -9.7772,    4.7565,\n",
      "             -9.5130],\n",
      "          ...,\n",
      "          [   2.1140,   -7.9275,    9.5130,  ...,   -6.6062,    4.2280,\n",
      "              2.6425],\n",
      "          [   2.1140,   -7.9275,    9.5130,  ...,  -10.0415,    5.0207,\n",
      "              2.6425],\n",
      "          [   4.2280,  -16.6477,   -6.0777,  ...,  -12.9482,   -0.7927,\n",
      "             -2.6425]]],\n",
      "\n",
      "\n",
      "        [[[  -6.8705,  -28.5390,    7.1347,  ...,    6.6062,    9.5130,\n",
      "              5.0207],\n",
      "          [  -3.1710,  -17.9690,   -1.3212,  ...,   12.9482,   16.6477,\n",
      "              7.3990],\n",
      "          [  -3.1710,   -2.9067,    7.9275,  ...,    4.4922,   17.1762,\n",
      "             12.1555],\n",
      "          ...,\n",
      "          [ -11.0985,    0.7927,  -18.4975,  ...,  -18.7617,  -28.2747,\n",
      "            -23.7825],\n",
      "          [   0.5285,   -2.1140,    0.0000,  ...,    1.0570,    1.0570,\n",
      "              1.3212],\n",
      "          [   7.1347,    8.4560,    8.4560,  ...,    8.4560,    8.4560,\n",
      "              8.1917]],\n",
      "\n",
      "         [[ -28.8032,  -37.5235,  -11.3627,  ...,  -18.2332,  -18.4975,\n",
      "            -18.4975],\n",
      "          [   6.6062,  -11.8912,  -22.9897,  ...,   -7.9275,  -16.1192,\n",
      "            -18.7617],\n",
      "          [   5.2850,   -7.9275,  -20.0830,  ...,   -5.0207,   -6.0777,\n",
      "            -17.4405],\n",
      "          ...,\n",
      "          [   3.9637,   29.3317,   62.8915,  ...,   59.7205,   63.4200,\n",
      "             70.2905],\n",
      "          [   3.4352,  -20.8757,  -35.9380,  ...,  -31.7100,  -39.6375,\n",
      "            -30.9172],\n",
      "          [   7.6632,    2.3782,    2.3782,  ...,    2.3782,    2.3782,\n",
      "              6.0777]],\n",
      "\n",
      "         [[ -86.6740, -148.5084,  -31.4457,  ...,  -37.7877,  -12.6840,\n",
      "             -0.7927],\n",
      "          [ -86.4097, -160.1354,  -20.0830,  ...,  -53.3785,  -19.8187,\n",
      "              1.0570],\n",
      "          [ -90.3735, -171.7624,  -27.2177,  ...,  -42.2800,  -34.3525,\n",
      "            -17.9690],\n",
      "          ...,\n",
      "          [ -35.6737,  -35.6737,  -37.2592,  ...,  -40.4302,  -33.0312,\n",
      "            -36.4665],\n",
      "          [ -31.4457,  -30.1245,  -29.5960,  ...,  -29.8602,  -30.1245,\n",
      "            -30.9172],\n",
      "          [ -33.8240,  -31.9742,  -31.9742,  ...,  -31.9742,  -31.9742,\n",
      "            -32.5027]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  26.4250,  -17.4405,    6.6062,  ...,    6.6062,    9.5130,\n",
      "             10.8342],\n",
      "          [  37.5235,  -16.3835,    2.3782,  ...,    3.9637,    7.3990,\n",
      "             11.3627],\n",
      "          [  34.8810,  -11.8912,    4.4922,  ...,    2.3782,   11.8912,\n",
      "              6.0777],\n",
      "          ...,\n",
      "          [  22.1970,   20.3472,   19.8187,  ...,   14.7980,   13.2125,\n",
      "             16.1192],\n",
      "          [  11.8912,    1.8497,    1.0570,  ...,    2.1140,    0.7927,\n",
      "             12.9482],\n",
      "          [   6.3420,    5.0207,    5.0207,  ...,    5.0207,    5.0207,\n",
      "             16.9120]],\n",
      "\n",
      "         [[   7.3990,  -22.4612,   -2.3782,  ...,    7.1347,   11.0985,\n",
      "             12.9482],\n",
      "          [  15.5907,  -14.7980,    1.3212,  ...,   -9.2487,    9.2487,\n",
      "             15.0622],\n",
      "          [   6.8705,  -38.5805,   -6.6062,  ...,   -3.4352,   -7.9275,\n",
      "              6.3420],\n",
      "          ...,\n",
      "          [ -20.3472,    4.4922,    5.0207,  ...,    1.8497,    1.8497,\n",
      "             12.1555],\n",
      "          [   1.0570,   -1.5855,   -3.1710,  ...,   -2.3782,   -5.0207,\n",
      "              2.1140],\n",
      "          [ -12.4197,   -5.8135,   -5.8135,  ...,   -5.8135,   -5.8135,\n",
      "              3.1710]],\n",
      "\n",
      "         [[ -18.2332,   10.0415,  -44.1297,  ...,  -15.0622,  -18.2332,\n",
      "             -3.6995],\n",
      "          [ -12.4197,  -12.9482,  -10.8342,  ...,    3.6995,  -17.9690,\n",
      "             -3.1710],\n",
      "          [  -7.6632,   -3.4352,  -13.4767,  ...,  -11.6270,    0.7927,\n",
      "             -3.9637],\n",
      "          ...,\n",
      "          [  14.0052,  -45.1867,  -38.0520,  ...,  -35.6737,   -9.7772,\n",
      "            -21.6685],\n",
      "          [  -8.4560,   -2.9067,   -5.8135,  ...,   -6.8705,   -8.7202,\n",
      "             -4.4922],\n",
      "          [  -0.5285,   -9.7772,   -9.7772,  ...,   -9.7772,   -9.7772,\n",
      "             -5.5492]]],\n",
      "\n",
      "\n",
      "        [[[  17.1762,   -4.2280,  -21.6685,  ...,   -4.7565,   -1.8497,\n",
      "              1.0570],\n",
      "          [  -1.0570,    6.8705,   12.1555,  ...,   -9.2487,   -4.7565,\n",
      "             -0.7927],\n",
      "          [ -20.6115,  -16.9120,   -3.9637,  ...,   -8.1917,   -4.7565,\n",
      "              0.0000],\n",
      "          ...,\n",
      "          [   6.6062,    2.9067,   18.4975,  ...,   23.2540,   19.8187,\n",
      "              2.6425],\n",
      "          [  11.6270,  -16.3835,  -10.0415,  ...,   12.4197,   21.9327,\n",
      "              2.1140],\n",
      "          [ -18.2332,  -27.2177,  -14.2695,  ...,   -3.6995,   19.0260,\n",
      "              4.4922]],\n",
      "\n",
      "         [[ -45.9795,  -56.5495,  -33.8240,  ...,  -17.4405,  -32.5027,\n",
      "            -29.8602],\n",
      "          [  25.8965,    0.5285,  -35.4095,  ...,  -17.4405,   -8.9845,\n",
      "              2.1140],\n",
      "          [ -16.3835,   25.6322,   43.3370,  ...,  -16.3835,   -8.7202,\n",
      "              2.9067],\n",
      "          ...,\n",
      "          [ -11.8912,  -20.6115,  -11.6270,  ...,  -25.8965,   -5.8135,\n",
      "              5.5492],\n",
      "          [  19.2902,   16.9120,   10.0415,  ...,  -22.9897,   -1.0570,\n",
      "              7.1347],\n",
      "          [  -6.3420,  -17.9690,  -15.3265,  ...,   -7.6632,    0.5285,\n",
      "             10.3057]],\n",
      "\n",
      "         [[ -38.8447,   -3.9637,   16.6477,  ...,   -1.0570,   47.8292,\n",
      "            -37.5235],\n",
      "          [ -52.5857,  -56.5495,  -17.4405,  ...,   -5.8135,   45.9795,\n",
      "            -33.0312],\n",
      "          [   8.9845,  -16.9120,  -35.9380,  ...,   -6.8705,   45.9795,\n",
      "            -33.5597],\n",
      "          ...,\n",
      "          [  -4.2280,    8.7202,    0.2642,  ...,   -1.3212,   44.1297,\n",
      "            -26.1607],\n",
      "          [   9.7772,  -12.4197,  -14.0052,  ...,   -5.8135,   44.9225,\n",
      "            -27.2177],\n",
      "          [  -8.9845,  -47.0365,   -3.9637,  ...,  -38.0520,   50.4717,\n",
      "            -29.5960]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  -3.6995,   -8.4560,   -3.4352,  ...,  -14.2695,   11.8912,\n",
      "             12.4197],\n",
      "          [   8.4560,   -0.2642,   -0.2642,  ...,  -11.0985,   22.1970,\n",
      "             23.5182],\n",
      "          [   7.3990,   10.5700,   19.0260,  ...,   -9.5130,   23.7825,\n",
      "             25.1037],\n",
      "          ...,\n",
      "          [  17.4405,   18.4975,   21.4042,  ...,   -1.3212,   37.7877,\n",
      "             26.1607],\n",
      "          [  35.1452,   16.6477,   14.7980,  ...,  -14.2695,   37.2592,\n",
      "             26.1607],\n",
      "          [  21.1400,   -1.8497,   11.0985,  ...,  -32.5027,   23.5182,\n",
      "             24.3110]],\n",
      "\n",
      "         [[  -8.1917,   12.6840,   28.0105,  ...,   -9.7772,   16.9120,\n",
      "              8.4560],\n",
      "          [ -17.1762,  -35.6737,  -34.0882,  ...,  -16.6477,   12.9482,\n",
      "             11.3627],\n",
      "          [  25.6322,    6.0777,  -16.3835,  ...,  -16.9120,   13.2125,\n",
      "             11.0985],\n",
      "          ...,\n",
      "          [ -33.0312,  -17.1762,   10.5700,  ...,    6.8705,   15.5907,\n",
      "              7.9275],\n",
      "          [  -1.3212,   20.6115,   15.5907,  ...,    2.9067,   16.3835,\n",
      "              8.1917],\n",
      "          [  29.3317,   11.6270,   15.3265,  ...,  -23.7825,   19.0260,\n",
      "              0.2642]],\n",
      "\n",
      "         [[  -6.0777,  -21.1400,  -23.2540,  ...,  -21.6685,  -24.0467,\n",
      "            -10.5700],\n",
      "          [   7.3990,    4.7565,   -0.7927,  ...,  -19.5545,  -30.3887,\n",
      "            -11.0985],\n",
      "          [ -43.8655,  -16.9120,  -22.9897,  ...,  -19.2902,  -30.1245,\n",
      "            -11.0985],\n",
      "          ...,\n",
      "          [ -13.4767,    8.7202,  -18.2332,  ...,   10.0415,    0.2642,\n",
      "             -7.6632],\n",
      "          [ -28.8032,  -12.9482,    0.0000,  ...,    7.3990,    5.8135,\n",
      "             -8.1917],\n",
      "          [ -20.8757,    2.6425,   -4.4922,  ...,   13.7410,    8.4560,\n",
      "              0.5285]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[   8.7202,    1.5855,    3.4352,  ...,    7.9275,    7.3990,\n",
      "              5.0207],\n",
      "          [  -1.5855,  -10.5700,   -7.3990,  ...,    3.9637,    4.4922,\n",
      "              4.2280],\n",
      "          [ -14.7980,   -6.6062,   -1.8497,  ...,    2.3782,    3.6995,\n",
      "              4.2280],\n",
      "          ...,\n",
      "          [ -38.8447,  -44.3940,  -44.9225,  ...,  -35.1452,  -13.2125,\n",
      "              4.2280],\n",
      "          [   8.7202,   11.8912,   11.8912,  ...,   -0.5285,    1.3212,\n",
      "              4.2280],\n",
      "          [   7.1347,    8.4560,    8.4560,  ...,    8.4560,    8.4560,\n",
      "              8.1917]],\n",
      "\n",
      "         [[ -18.4975,  -34.8810,  -46.7722,  ...,  -42.5442,  -47.0365,\n",
      "            -38.5805],\n",
      "          [   8.1917,    7.6632,    6.0777,  ...,   -2.1140,   -0.2642,\n",
      "              1.3212],\n",
      "          [ -11.0985,  -18.7617,  -10.3057,  ...,    0.0000,    1.0570,\n",
      "              1.3212],\n",
      "          ...,\n",
      "          [ 101.7362,  109.3994,  110.7207,  ...,   54.9640,   13.4767,\n",
      "              1.3212],\n",
      "          [ -52.0572,  -54.4355,  -54.4355,  ...,  -24.8395,   -8.1917,\n",
      "              1.3212],\n",
      "          [   7.6632,    2.3782,    2.3782,  ...,    2.3782,    2.3782,\n",
      "              6.0777]],\n",
      "\n",
      "         [[ -31.7100,  -31.1815,  -52.3215,  ...,   16.9120,  -34.0882,\n",
      "            -34.0882],\n",
      "          [ -27.7462,  -32.2385,  -41.4872,  ...,   19.0260,  -29.8602,\n",
      "            -29.8602],\n",
      "          [ -52.3215,  -24.5752,  -22.4612,  ...,   16.3835,  -30.9172,\n",
      "            -29.8602],\n",
      "          ...,\n",
      "          [ -62.6272,  -61.3060,  -56.8137,  ...,  -33.8240,  -38.8447,\n",
      "            -29.8602],\n",
      "          [ -37.7877,  -40.1660,  -39.3732,  ...,  -32.7670,  -32.7670,\n",
      "            -29.8602],\n",
      "          [ -33.8240,  -31.9742,  -31.9742,  ...,  -31.9742,  -31.9742,\n",
      "            -32.5027]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[   5.2850,   15.0622,    1.5855,  ...,    8.9845,   -0.7927,\n",
      "              9.2487],\n",
      "          [  13.2125,   18.2332,    3.9637,  ...,   18.4975,   11.3627,\n",
      "             21.6685],\n",
      "          [  -1.8497,   10.3057,    1.0570,  ...,   17.4405,   10.3057,\n",
      "             21.6685],\n",
      "          ...,\n",
      "          [  21.6685,   17.1762,   16.9120,  ...,    5.0207,    8.4560,\n",
      "             21.6685],\n",
      "          [  -2.3782,   -5.2850,   -5.0207,  ...,    5.2850,   11.0985,\n",
      "             21.6685],\n",
      "          [   6.3420,    5.0207,    5.0207,  ...,    5.0207,    5.0207,\n",
      "             16.9120]],\n",
      "\n",
      "         [[  -9.5130,   -3.1710,  -12.1555,  ...,    6.6062,   -3.6995,\n",
      "              2.3782],\n",
      "          [   8.7202,   11.3627,  -13.7410,  ...,    5.0207,   -1.8497,\n",
      "              5.5492],\n",
      "          [  13.7410,   10.8342,   -5.8135,  ...,    3.6995,   -2.6425,\n",
      "              5.5492],\n",
      "          ...,\n",
      "          [ -12.9482,  -14.5337,  -14.0052,  ...,  -10.8342,   -5.0207,\n",
      "              5.5492],\n",
      "          [   1.5855,   -1.5855,   -1.3212,  ...,   -6.3420,   -3.4352,\n",
      "              5.5492],\n",
      "          [ -12.4197,   -5.8135,   -5.8135,  ...,   -5.8135,   -5.8135,\n",
      "              3.1710]],\n",
      "\n",
      "         [[   8.1917,    0.0000,    7.6632,  ...,  -11.6270,  -11.8912,\n",
      "             -8.1917],\n",
      "          [ -18.4975,  -13.7410,   -2.9067,  ...,  -12.9482,   -9.7772,\n",
      "             -5.8135],\n",
      "          [   2.6425,  -11.0985,  -10.0415,  ...,  -10.0415,   -7.9275,\n",
      "             -5.8135],\n",
      "          ...,\n",
      "          [ -24.8395,  -25.6322,  -23.2540,  ...,   15.0622,   -3.4352,\n",
      "             -5.8135],\n",
      "          [ -19.8187,  -21.4042,  -20.6115,  ...,   -9.7772,  -16.9120,\n",
      "             -5.8135],\n",
      "          [  -0.5285,   -9.7772,   -9.7772,  ...,   -9.7772,   -9.7772,\n",
      "             -5.5492]]],\n",
      "\n",
      "\n",
      "        [[[   0.5285,    6.0777,   -0.5285,  ...,    4.4922,    3.1710,\n",
      "              7.1347],\n",
      "          [ -12.6840,   -3.6995,  -18.7617,  ...,    3.9637,    1.0570,\n",
      "              5.0207],\n",
      "          [  12.4197,   -6.6062,  -11.8912,  ...,   -1.0570,    0.0000,\n",
      "              4.7565],\n",
      "          ...,\n",
      "          [   2.9067,    0.2642,    6.6062,  ...,   -2.1140,    6.3420,\n",
      "              4.2280],\n",
      "          [  36.2022,   44.3940,   28.5390,  ...,   41.2230,    9.7772,\n",
      "             10.3057],\n",
      "          [ -23.7825,  -25.6322,  -30.1245,  ...,   -6.0777,  -14.2695,\n",
      "              4.2280]],\n",
      "\n",
      "         [[ -32.5027,  -42.2800,  -44.1297,  ...,  -22.7255,  -32.2385,\n",
      "            -29.8602],\n",
      "          [  -6.3420,    5.0207,    7.9275,  ...,   -4.4922,    2.3782,\n",
      "              8.1917],\n",
      "          [ -25.3680,  -21.6685,  -24.3110,  ...,   -2.6425,    2.6425,\n",
      "              7.9275],\n",
      "          ...,\n",
      "          [ -33.0312,  -27.4820,   -8.1917,  ...,  -10.3057,    1.8497,\n",
      "              6.8705],\n",
      "          [ -17.9690,  -22.9897,  -19.5545,  ...,   -2.6425,   -0.2642,\n",
      "              4.4922],\n",
      "          [  93.8087,   99.0937,   78.2180,  ...,   63.1557,   28.2747,\n",
      "             14.0052]],\n",
      "\n",
      "         [[ -40.6945,  -72.1402,  -75.5755,  ...,  -10.5700,   32.7670,\n",
      "            -36.4665],\n",
      "          [ -21.4042,  -54.4355,  -99.3579,  ...,   -4.2280,   23.2540,\n",
      "            -36.2022],\n",
      "          [ -12.6840,  -45.9795, -108.0782,  ...,   -3.4352,   19.5545,\n",
      "            -38.0520],\n",
      "          ...,\n",
      "          [ -15.5907,   12.6840,    5.2850,  ...,  -12.1555,   26.9535,\n",
      "            -32.5027],\n",
      "          [ -44.1297,  -11.3627,    5.8135,  ...,  -14.0052,   22.9897,\n",
      "            -26.4250],\n",
      "          [ -47.5650,  -36.9950,  -35.4095,  ...,  -36.2022,  -31.1815,\n",
      "            -36.4665]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[   3.4352,    1.5855,   -0.2642,  ...,   -6.3420,   11.8912,\n",
      "             15.0622],\n",
      "          [   4.2280,   16.1192,    5.8135,  ...,   -0.2642,   21.1400,\n",
      "             21.9327],\n",
      "          [   5.5492,    7.6632,    3.4352,  ...,    0.0000,   21.6685,\n",
      "             21.6685],\n",
      "          ...,\n",
      "          [  12.6840,   18.7617,   32.5027,  ...,   -4.7565,   18.2332,\n",
      "             21.6685],\n",
      "          [  10.5700,   18.7617,   30.9172,  ...,    6.8705,   14.0052,\n",
      "             22.1970],\n",
      "          [   9.5130,    8.7202,   15.5907,  ...,   -0.7927,    0.2642,\n",
      "             15.5907]],\n",
      "\n",
      "         [[   1.3212,   -3.9637,   -2.1140,  ...,  -19.2902,    4.7565,\n",
      "              3.4352],\n",
      "          [  14.2695,   17.4405,   18.2332,  ...,  -17.1762,    1.3212,\n",
      "              6.0777],\n",
      "          [  -7.9275,   24.5752,    0.0000,  ...,  -11.3627,    2.6425,\n",
      "              3.4352],\n",
      "          ...,\n",
      "          [  -5.2850,   10.8342,    7.6632,  ...,  -27.2177,   -9.7772,\n",
      "             -1.5855],\n",
      "          [ -66.5910,  -57.8707,  -23.7825,  ...,  -45.4510,    7.9275,\n",
      "              8.9845],\n",
      "          [  -8.4560,    1.5855,   -3.6995,  ...,   -2.6425,  -10.3057,\n",
      "              4.2280]],\n",
      "\n",
      "         [[  -1.5855,   -6.3420,    5.0207,  ...,  -14.5337,   -2.1140,\n",
      "             -6.0777],\n",
      "          [  -7.9275,   -2.6425,  -19.5545,  ...,  -12.9482,   -8.4560,\n",
      "             -0.7927],\n",
      "          [  -2.3782,  -17.4405,  -11.0985,  ...,   -9.7772,   -9.2487,\n",
      "             -0.2642],\n",
      "          ...,\n",
      "          [  14.0052,    9.2487,    3.4352,  ...,    6.6062,    0.0000,\n",
      "              6.0777],\n",
      "          [  -0.2642,   13.7410,    1.3212,  ...,   -8.7202,  -19.5545,\n",
      "             -9.7772],\n",
      "          [  -8.4560,  -21.6685,    2.9067,  ...,  -21.1400,   13.7410,\n",
      "             -5.5492]]],\n",
      "\n",
      "\n",
      "        [[[ -29.5960,   27.7462,   32.7670,  ...,   53.3785,   42.2800,\n",
      "             27.7462],\n",
      "          [ -13.2125,   12.1555,   12.1555,  ...,    6.3420,   21.4042,\n",
      "             12.9482],\n",
      "          [ -21.6685,    1.3212,    4.2280,  ...,  -10.8342,   -6.8705,\n",
      "              4.7565],\n",
      "          ...,\n",
      "          [ -23.7825,   10.3057,    9.7772,  ...,    0.2642,    0.5285,\n",
      "              2.1140],\n",
      "          [ -24.5752,    2.9067,    4.2280,  ...,    5.2850,    5.0207,\n",
      "             11.8912],\n",
      "          [ -23.5182,  -11.8912,   -6.0777,  ...,   -9.7772,   -3.1710,\n",
      "             -3.1710]],\n",
      "\n",
      "         [[ -90.9019, -122.8762, -122.8762,  ..., -120.7622, -124.1974,\n",
      "           -118.6482],\n",
      "          [  11.8912,   14.0052,   15.3265,  ...,   48.8862,   37.2592,\n",
      "             14.7980],\n",
      "          [  -3.9637,  -16.1192,  -19.8187,  ...,   -3.6995,   -9.5130,\n",
      "             -5.2850],\n",
      "          ...,\n",
      "          [  -2.9067,   -7.6632,  -12.9482,  ...,  -15.3265,  -17.1762,\n",
      "            -24.0467],\n",
      "          [  -6.0777,  -12.1555,  -17.1762,  ...,  -25.1037,  -22.4612,\n",
      "            -30.1245],\n",
      "          [ -10.5700,  -20.8757,  -23.5182,  ...,  -30.1245,  -22.1970,\n",
      "            -21.6685]],\n",
      "\n",
      "         [[-161.9852,  -50.4717,  -38.3162,  ...,  -45.9795,  -60.7775,\n",
      "            -45.7152],\n",
      "          [-177.3116,  -41.4872,  -29.8602,  ...,  -59.7205,  -81.3890,\n",
      "            -74.7827],\n",
      "          [-180.7469,  -46.7722,  -29.5960,  ...,  -99.8864,  -73.9900,\n",
      "            -84.2957],\n",
      "          ...,\n",
      "          [-170.9697,  -38.8447,  -28.2747,  ...,  -31.9742,  -25.6322,\n",
      "            -26.6892],\n",
      "          [-167.0059,  -35.4095,  -36.4665,  ...,  -26.6892,  -28.8032,\n",
      "            -29.8602],\n",
      "          [-171.4982,  -38.0520,  -38.5805,  ...,  -22.7255,  -20.0830,\n",
      "            -21.4042]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ -36.9950,  -12.1555,  -18.4975,  ...,    0.5285,  -10.5700,\n",
      "             -3.9637],\n",
      "          [ -12.6840,    6.0777,    4.4922,  ...,   14.5337,   16.9120,\n",
      "              7.9275],\n",
      "          [ -18.7617,    2.1140,    3.1710,  ...,   10.0415,    8.7202,\n",
      "             12.4197],\n",
      "          ...,\n",
      "          [ -18.2332,    7.6632,    6.6062,  ...,    5.5492,    4.7565,\n",
      "              7.1347],\n",
      "          [ -20.3472,    6.8705,    3.4352,  ...,    4.7565,    4.4922,\n",
      "              7.9275],\n",
      "          [ -15.8550,    9.7772,   14.5337,  ...,   14.2695,   19.0260,\n",
      "             19.5545]],\n",
      "\n",
      "         [[ -33.5597,  -14.2695,   -9.7772,  ...,  -26.4250,  -15.8550,\n",
      "             -0.2642],\n",
      "          [ -35.1452,  -12.1555,   -5.2850,  ...,   -1.0570,  -20.0830,\n",
      "            -22.1970],\n",
      "          [ -26.4250,   -6.6062,    3.4352,  ...,  -17.9690,   10.5700,\n",
      "            -18.2332],\n",
      "          ...,\n",
      "          [ -29.0675,  -10.0415,    8.9845,  ...,    5.0207,   11.0985,\n",
      "              4.2280],\n",
      "          [ -28.8032,   -0.2642,    8.7202,  ...,    8.1917,    1.8497,\n",
      "             -7.6632],\n",
      "          [ -25.8965,   14.0052,    9.5130,  ...,    7.9275,   -1.0570,\n",
      "              1.5855]],\n",
      "\n",
      "         [[  38.3162,  -19.0260,   -8.7202,  ...,    6.6062,   -1.5855,\n",
      "            -30.6530],\n",
      "          [ -15.5907,  -22.7255,  -16.1192,  ...,  -11.6270,   -4.2280,\n",
      "            -10.5700],\n",
      "          [  -2.6425,   -0.5285,    2.1140,  ...,   24.5752,   -8.1917,\n",
      "              5.0207],\n",
      "          ...,\n",
      "          [   1.0570,   -5.2850,    0.2642,  ...,   -2.3782,   -7.6632,\n",
      "             -2.3782],\n",
      "          [   1.8497,   -4.4922,    3.1710,  ...,   -8.7202,   -4.2280,\n",
      "              0.7927],\n",
      "          [  -6.8705,  -12.4197,    0.2642,  ...,    0.5285,    1.5855,\n",
      "             -0.5285]]]], device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2nd convolution's input size: torch.Size([128, 16, 32, 32])\n",
    "conv_int = torch.nn.Conv2d(in_channels = 128, out_channels=16, kernel_size = 3, bias = False)\n",
    "conv_int.weight = torch.nn.Parameter(weight_int)\n",
    "\n",
    "output_int =  conv_int(x_int)    # output_int can be calculated with conv_int and x_int\n",
    "output_recovered = output_int * x_delta * w_delta  # recover with x_delta and w_delta\n",
    "print(output_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "designed-auction",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "designed-auction",
    "outputId": "62cb7082-d65e-4c24-82e0-829754875bf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.7203e+01,  3.5003e+01, -3.2401e+01,  ...,  3.7670e+01,\n",
      "            5.0941e+01,  1.7690e+01],\n",
      "          [ 8.2891e-01,  1.3824e+01, -1.3264e+01,  ...,  5.6818e-01,\n",
      "           -1.7833e+01, -1.5209e+01],\n",
      "          [-3.5553e-02,  1.0115e+01, -1.8701e+01,  ..., -3.3465e+00,\n",
      "           -1.5945e+01, -1.1722e+00],\n",
      "          ...,\n",
      "          [-2.0300e-02,  1.0082e+01, -1.8897e+01,  ..., -1.2483e+00,\n",
      "            3.1761e+00,  1.2059e+01],\n",
      "          [-2.0300e-02,  1.0082e+01, -1.8897e+01,  ..., -5.7076e+00,\n",
      "            3.0425e+00,  1.3358e+01],\n",
      "          [-2.2341e+00,  9.8527e+00, -3.1873e+01,  ..., -8.0737e+00,\n",
      "            1.6614e+00,  1.1037e+01]],\n",
      "\n",
      "         [[-4.2258e+01, -4.9316e+01, -1.2625e+02,  ..., -1.0211e+02,\n",
      "           -9.1568e+01, -9.2683e+01],\n",
      "          [ 5.9006e+00,  8.3631e+00,  2.0084e+01,  ...,  6.3916e+01,\n",
      "            4.8938e+01,  4.9815e+01],\n",
      "          [ 4.1510e+00, -1.1522e+01, -2.3205e+01,  ..., -6.9784e+00,\n",
      "           -1.1996e+01, -2.2105e+01],\n",
      "          ...,\n",
      "          [ 4.2416e+00, -1.1583e+01, -2.3812e+01,  ..., -7.8615e+00,\n",
      "           -5.8401e+00, -1.6844e+01],\n",
      "          [ 4.2416e+00, -1.1583e+01, -2.3812e+01,  ..., -8.7630e+00,\n",
      "           -6.9189e+00, -1.6428e+01],\n",
      "          [ 8.1707e+00, -8.6872e+00, -3.5331e+01,  ..., -1.3702e+01,\n",
      "           -1.1363e+01, -2.0231e+01]],\n",
      "\n",
      "         [[-3.8846e+01, -1.5455e+02, -2.5404e+02,  ..., -5.4020e+01,\n",
      "           -2.1914e+01, -1.4912e+02],\n",
      "          [-4.0538e+01, -1.5679e+02, -2.8177e+02,  ..., -5.3927e+01,\n",
      "           -6.0830e+01, -1.6198e+02],\n",
      "          [-4.0215e+01, -1.5528e+02, -2.8311e+02,  ..., -5.1196e+01,\n",
      "           -8.1692e+01, -1.6930e+02],\n",
      "          ...,\n",
      "          [-4.0189e+01, -1.5529e+02, -2.8319e+02,  ..., -3.4498e+01,\n",
      "           -4.0844e+01, -1.0737e+02],\n",
      "          [-4.0189e+01, -1.5529e+02, -2.8319e+02,  ..., -4.2369e+01,\n",
      "           -3.8407e+01, -1.0671e+02],\n",
      "          [-3.8765e+01, -1.5345e+02, -2.8679e+02,  ..., -5.3581e+01,\n",
      "           -3.6650e+01, -1.0545e+02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5070e+01,  2.7968e+01, -6.7667e+01,  ..., -1.2328e+01,\n",
      "            1.1504e+01,  4.1951e+00],\n",
      "          [ 3.4523e+01,  5.5399e+01, -3.8316e+01,  ...,  2.2973e+01,\n",
      "            2.7191e+01,  3.1734e+01],\n",
      "          [ 3.8134e+01,  5.3055e+01, -4.7954e+01,  ...,  1.0528e+01,\n",
      "            2.2833e+01,  1.7230e+01],\n",
      "          ...,\n",
      "          [ 3.8235e+01,  5.3085e+01, -4.8283e+01,  ...,  3.5393e+00,\n",
      "            1.6902e+01,  2.1522e+01],\n",
      "          [ 3.8235e+01,  5.3085e+01, -4.8283e+01,  ...,  6.2980e-01,\n",
      "            1.6675e+01,  2.1878e+01],\n",
      "          [ 3.4935e+01,  5.1949e+01, -4.2384e+01,  ..., -4.7314e+00,\n",
      "            1.3564e+01,  2.7929e+01]],\n",
      "\n",
      "         [[ 2.4279e+01,  3.8057e+01, -5.2511e+01,  ..., -3.7352e+01,\n",
      "            1.8196e+01, -1.6036e+01],\n",
      "          [ 1.4297e+01,  2.8718e+01, -3.9525e+01,  ...,  7.1596e-02,\n",
      "            2.4909e+01,  7.1743e+00],\n",
      "          [ 1.4255e+01,  2.8170e+01, -4.1143e+01,  ...,  1.1864e+01,\n",
      "            1.4525e+01, -1.5348e+01],\n",
      "          ...,\n",
      "          [ 1.4341e+01,  2.8278e+01, -4.1226e+01,  ...,  7.3687e+00,\n",
      "            1.0745e+01, -1.2020e+00],\n",
      "          [ 1.4341e+01,  2.8278e+01, -4.1226e+01,  ...,  3.0128e+00,\n",
      "            1.2157e+01, -1.8231e+00],\n",
      "          [ 9.7758e+00,  2.4155e+01, -3.5330e+01,  ..., -3.6394e+00,\n",
      "            6.8776e+00,  5.8177e-01]],\n",
      "\n",
      "         [[ 1.1343e+01, -3.7945e+01,  6.4079e+01,  ...,  2.5491e+01,\n",
      "           -7.3677e+00, -4.9686e+01],\n",
      "          [ 6.0875e+00, -4.9673e+00, -2.9883e+01,  ..., -1.6328e+01,\n",
      "           -5.5984e+00, -1.5640e+01],\n",
      "          [ 4.8106e+00,  1.1366e+01,  5.6484e+00,  ..., -1.1656e+01,\n",
      "            1.2736e+01,  3.0745e+00],\n",
      "          ...,\n",
      "          [ 4.7382e+00,  1.1331e+01,  6.0455e+00,  ..., -6.7807e+00,\n",
      "            6.4857e+00,  1.2472e+01],\n",
      "          [ 4.7382e+00,  1.1331e+01,  6.0455e+00,  ..., -8.7043e+00,\n",
      "            5.2761e+00,  1.1657e+01],\n",
      "          [ 7.9187e+00,  5.6819e-01, -8.2342e+00,  ..., -1.5024e+01,\n",
      "            2.8945e+00,  5.5868e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.2306e+00, -3.5531e+01,  4.1081e+00,  ...,  7.5564e+00,\n",
      "            1.2108e+01,  6.4146e+00],\n",
      "          [ 1.3546e+00, -2.9936e+01, -5.2378e+00,  ...,  1.1803e+01,\n",
      "            1.4806e+01,  1.0393e+01],\n",
      "          [-6.1980e+00, -6.3586e+00,  3.4666e-01,  ...,  3.9964e+00,\n",
      "            1.5245e+01,  1.1492e+01],\n",
      "          ...,\n",
      "          [-1.2975e+01, -1.9026e+00, -2.2810e+01,  ..., -2.3404e+01,\n",
      "           -3.1920e+01, -2.6665e+01],\n",
      "          [-3.1155e+00, -2.6601e+00,  1.4945e+00,  ...,  9.0620e-02,\n",
      "            8.1954e-01,  3.9231e-01],\n",
      "          [ 1.0025e+00,  3.4307e-01,  3.4307e-01,  ...,  3.4307e-01,\n",
      "            3.4307e-01, -1.4427e+00]],\n",
      "\n",
      "         [[-2.5503e+01, -4.0773e+01, -1.4135e+01,  ..., -1.7420e+01,\n",
      "           -1.7498e+01, -1.9186e+01],\n",
      "          [ 7.4626e+00, -1.7572e+01, -2.8533e+01,  ..., -1.3030e+01,\n",
      "           -1.9667e+01, -1.9982e+01],\n",
      "          [ 4.7380e+00, -1.3739e+01, -2.7956e+01,  ..., -5.4678e+00,\n",
      "           -5.6093e+00, -1.6949e+01],\n",
      "          ...,\n",
      "          [ 4.4238e+00,  3.6893e+01,  7.3215e+01,  ...,  6.9237e+01,\n",
      "            7.2747e+01,  7.6792e+01],\n",
      "          [ 3.7773e+00, -2.0161e+01, -3.4209e+01,  ..., -3.1740e+01,\n",
      "           -3.7390e+01, -2.8943e+01],\n",
      "          [ 5.7600e+00,  1.4538e+00,  1.4538e+00,  ...,  1.4538e+00,\n",
      "            1.4538e+00,  4.9861e+00]],\n",
      "\n",
      "         [[-1.0184e+02, -1.6556e+02, -2.7646e+01,  ..., -3.6670e+01,\n",
      "           -1.0587e+01,  2.0543e+00],\n",
      "          [-1.0596e+02, -1.8494e+02, -7.4998e+00,  ..., -5.1628e+01,\n",
      "           -1.8051e+01,  4.7271e+00],\n",
      "          [-1.0810e+02, -2.0154e+02, -1.2723e+01,  ..., -4.2394e+01,\n",
      "           -3.2810e+01, -1.6074e+01],\n",
      "          ...,\n",
      "          [-3.2668e+01, -3.7121e+01, -4.1697e+01,  ..., -4.2439e+01,\n",
      "           -3.7174e+01, -3.9381e+01],\n",
      "          [-3.0758e+01, -2.9666e+01, -2.9472e+01,  ..., -2.9419e+01,\n",
      "           -2.9450e+01, -3.0666e+01],\n",
      "          [-2.9669e+01, -2.8642e+01, -2.8642e+01,  ..., -2.8642e+01,\n",
      "           -2.8642e+01, -3.0478e+01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.1512e+01, -2.8487e+01,  8.4051e+00,  ...,  2.7363e+00,\n",
      "            8.0018e+00,  8.6899e+00],\n",
      "          [ 4.9123e+01, -3.1761e+01, -6.3489e-01,  ...,  1.5013e+00,\n",
      "            5.2811e+00,  7.9206e+00],\n",
      "          [ 4.2084e+01, -2.4306e+01,  2.1133e+00,  ...,  1.3787e+00,\n",
      "            7.5981e+00,  3.3897e+00],\n",
      "          ...,\n",
      "          [ 2.5618e+01,  2.2364e+01,  1.7083e+01,  ...,  1.3965e+01,\n",
      "            8.6484e+00,  1.3869e+01],\n",
      "          [ 1.1823e+01, -3.7473e+00, -3.8331e+00,  ..., -3.4671e+00,\n",
      "           -4.7484e+00,  1.1176e+01],\n",
      "          [ 7.2518e+00,  8.9498e-01,  8.9498e-01,  ...,  8.9498e-01,\n",
      "            8.9498e-01,  1.5803e+01]],\n",
      "\n",
      "         [[ 9.1507e+00, -2.4486e+01, -1.3058e+00,  ...,  3.6102e+00,\n",
      "            8.9119e+00,  1.2502e+01],\n",
      "          [ 2.1084e+01, -1.7599e+01,  1.7798e+00,  ..., -1.1635e+01,\n",
      "            6.9678e+00,  1.4986e+01],\n",
      "          [ 1.9050e+01, -5.0061e+01, -8.3873e+00,  ..., -3.3188e+00,\n",
      "           -7.4855e+00,  5.4618e+00],\n",
      "          ...,\n",
      "          [-1.5006e+01,  1.2640e+01,  7.5873e+00,  ...,  6.9630e+00,\n",
      "            4.3087e+00,  1.5260e+01],\n",
      "          [ 1.1958e+00, -7.4586e-01, -2.2721e+00,  ..., -3.1188e+00,\n",
      "           -5.8455e+00,  3.2260e+00],\n",
      "          [-1.2111e+01, -9.3915e+00, -9.3915e+00,  ..., -9.3915e+00,\n",
      "           -9.3915e+00,  1.3196e+00]],\n",
      "\n",
      "         [[-2.1839e+01,  1.1015e+01, -5.0609e+01,  ..., -1.0552e+01,\n",
      "           -1.6613e+01, -3.7344e+00],\n",
      "          [-1.8058e+01, -1.1496e+01, -2.1404e+01,  ...,  2.3012e+00,\n",
      "           -1.5159e+01,  9.0634e-01],\n",
      "          [-1.0251e+01,  6.8377e-01, -1.7786e+01,  ..., -8.8316e+00,\n",
      "            1.1908e+00, -9.7699e-01],\n",
      "          ...,\n",
      "          [ 6.4998e+00, -5.0172e+01, -4.3306e+01,  ..., -4.0989e+01,\n",
      "           -1.1048e+01, -1.7110e+01],\n",
      "          [-1.5424e+01, -4.3299e+00, -8.2711e+00,  ..., -9.5218e+00,\n",
      "           -1.0441e+01, -3.5295e+00],\n",
      "          [-4.2715e+00, -1.0045e+01, -1.0045e+01,  ..., -1.0045e+01,\n",
      "           -1.0045e+01, -3.8049e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2884e+01, -6.8976e+00, -2.3995e+01,  ..., -1.0778e+01,\n",
      "           -9.8575e+00,  6.4528e-01],\n",
      "          [-4.4613e+00,  2.0185e+00,  9.4062e+00,  ..., -1.1395e+01,\n",
      "           -7.9519e+00, -4.9168e+00],\n",
      "          [-2.8039e+01, -1.9086e+01, -6.3864e+00,  ..., -1.2111e+01,\n",
      "           -7.6181e+00, -5.0194e+00],\n",
      "          ...,\n",
      "          [ 2.2844e+00,  4.9249e+00,  1.8772e+01,  ...,  2.6308e+01,\n",
      "            2.0235e+01, -1.3619e+00],\n",
      "          [ 1.0631e+01, -1.5811e+01, -1.2856e+01,  ...,  5.2859e+00,\n",
      "            2.0996e+01, -2.0272e+00],\n",
      "          [-1.6162e+01, -2.7856e+01, -1.6584e+01,  ..., -7.4351e+00,\n",
      "            1.7130e+01, -1.4340e+00]],\n",
      "\n",
      "         [[-4.9241e+01, -5.8019e+01, -3.1748e+01,  ..., -1.4391e+01,\n",
      "           -3.3680e+01, -3.0767e+01],\n",
      "          [ 2.8784e+01,  3.0687e+00, -3.3995e+01,  ..., -2.1735e+01,\n",
      "           -1.0909e+01,  6.3514e+00],\n",
      "          [-1.3894e+01,  2.7173e+01,  4.6583e+01,  ..., -2.1760e+01,\n",
      "           -1.0885e+01,  6.3099e+00],\n",
      "          ...,\n",
      "          [-1.4219e+01, -2.5550e+01, -1.5413e+01,  ..., -3.9652e+01,\n",
      "           -9.3952e+00,  7.8003e+00],\n",
      "          [ 1.7194e+01,  1.7587e+01,  9.2883e+00,  ..., -3.8483e+01,\n",
      "           -9.7043e+00,  8.6835e+00],\n",
      "          [-2.3481e+00, -1.9011e+01, -1.6922e+01,  ..., -3.3465e+01,\n",
      "           -1.6131e+01,  1.0658e+01]],\n",
      "\n",
      "         [[-3.8579e+01, -4.3310e-01,  2.3341e+01,  ..., -8.4371e+00,\n",
      "            5.9310e+01, -3.6704e+01],\n",
      "          [-5.2073e+01, -5.7127e+01, -1.7307e+01,  ..., -9.8422e+00,\n",
      "            5.5092e+01, -3.4235e+01],\n",
      "          [ 6.3138e+00, -1.8356e+01, -3.9020e+01,  ..., -9.8705e+00,\n",
      "            5.4653e+01, -3.4358e+01],\n",
      "          ...,\n",
      "          [-6.0081e+00,  9.4493e+00,  2.2003e+00,  ..., -4.3277e+00,\n",
      "            5.5183e+01, -2.7238e+01],\n",
      "          [ 1.1974e+01, -7.9089e+00, -1.2059e+01,  ..., -1.5189e+01,\n",
      "            6.4048e+01, -2.6596e+01],\n",
      "          [-1.1846e+01, -4.3357e+01, -2.6924e+00,  ..., -6.2185e+01,\n",
      "            8.8484e+01, -2.7195e+01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.3416e+00, -8.3959e+00, -5.7651e+00,  ..., -1.9086e+01,\n",
      "            1.1345e+01,  1.4403e+01],\n",
      "          [ 7.6663e+00, -9.1320e-01, -2.8371e+00,  ..., -1.4462e+01,\n",
      "            2.5282e+01,  2.5611e+01],\n",
      "          [ 5.8321e+00,  1.2063e+01,  1.7177e+01,  ..., -1.3879e+01,\n",
      "            2.5641e+01,  2.5530e+01],\n",
      "          ...,\n",
      "          [ 1.3881e+01,  1.7092e+01,  2.1533e+01,  ..., -5.4309e+00,\n",
      "            4.1251e+01,  2.7215e+01],\n",
      "          [ 3.5294e+01,  1.6149e+01,  1.2391e+01,  ..., -2.4114e+01,\n",
      "            4.0077e+01,  2.7513e+01],\n",
      "          [ 2.0281e+01, -2.6108e+00,  1.0977e+01,  ..., -5.1925e+01,\n",
      "            2.8288e+01,  2.5330e+01]],\n",
      "\n",
      "         [[-1.6867e+01,  8.9375e+00,  3.0287e+01,  ..., -6.6052e+00,\n",
      "            1.1520e+01,  7.9669e+00],\n",
      "          [-1.4496e+01, -3.5067e+01, -3.8179e+01,  ..., -1.4987e+01,\n",
      "            1.1569e+01,  8.7684e+00],\n",
      "          [ 2.9351e+01,  7.0801e+00, -1.9698e+01,  ..., -1.3599e+01,\n",
      "            1.1842e+01,  8.7585e+00],\n",
      "          ...,\n",
      "          [-3.6849e+01, -1.8333e+01,  1.0953e+01,  ...,  6.9269e+00,\n",
      "            1.7725e+01,  6.3581e+00],\n",
      "          [-2.4759e+00,  2.1044e+01,  1.6646e+01,  ...,  6.7145e+00,\n",
      "            1.6217e+01,  5.6588e+00],\n",
      "          [ 2.7303e+01,  1.7162e+01,  1.7167e+01,  ..., -2.5238e+01,\n",
      "            2.1398e+01, -2.7769e+00]],\n",
      "\n",
      "         [[-4.2860e+00, -2.1395e+01, -2.7246e+01,  ..., -3.0984e+01,\n",
      "           -1.2696e+01, -1.0787e+01],\n",
      "          [ 5.7553e+00,  4.3042e+00,  1.2001e+00,  ..., -2.2867e+01,\n",
      "           -3.2172e+01, -6.3642e+00],\n",
      "          [-4.1612e+01, -1.6760e+01, -2.5521e+01,  ..., -2.3732e+01,\n",
      "           -3.2337e+01, -6.2163e+00],\n",
      "          ...,\n",
      "          [-9.1560e+00,  1.0727e+01, -2.0441e+01,  ...,  1.3393e+01,\n",
      "           -3.9038e+00, -5.8687e+00],\n",
      "          [-2.7516e+01, -1.3486e+01, -5.1025e-02,  ...,  2.7984e+00,\n",
      "           -3.2678e-01, -4.8176e+00],\n",
      "          [-1.5331e+01,  5.7434e-01, -4.2839e+00,  ...,  2.0417e+01,\n",
      "           -1.0824e+01,  3.9758e+00]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 7.6106e+00,  1.9326e+00,  1.1596e+00,  ...,  8.6662e+00,\n",
      "            8.0316e+00,  6.1043e+00],\n",
      "          [-5.1005e+00, -1.3177e+01, -5.9749e+00,  ...,  9.8557e-01,\n",
      "           -3.2342e-01, -1.0594e+00],\n",
      "          [-2.0422e+01, -8.2840e+00, -4.0476e+00,  ..., -1.7998e+00,\n",
      "           -7.0109e-01, -1.0594e+00],\n",
      "          ...,\n",
      "          [-5.4320e+01, -6.3045e+01, -6.5857e+01,  ..., -5.1706e+01,\n",
      "           -1.5987e+01, -1.0594e+00],\n",
      "          [ 1.1920e+01,  1.3285e+01,  1.2508e+01,  ..., -3.0176e+00,\n",
      "           -1.6046e+00, -1.0594e+00],\n",
      "          [ 1.0025e+00,  3.4307e-01,  3.4307e-01,  ...,  3.4307e-01,\n",
      "            3.4307e-01, -1.4427e+00]],\n",
      "\n",
      "         [[-1.8721e+01, -3.3553e+01, -4.5918e+01,  ..., -4.0671e+01,\n",
      "           -4.3969e+01, -3.7873e+01],\n",
      "          [ 6.9626e+00,  8.9411e+00,  8.6313e+00,  ...,  1.0909e+00,\n",
      "            4.1554e+00,  2.7794e+00],\n",
      "          [-9.5802e+00, -1.8773e+01, -8.7934e+00,  ...,  1.4862e+00,\n",
      "            4.2343e+00,  2.7794e+00],\n",
      "          ...,\n",
      "          [ 1.3313e+02,  1.5895e+02,  1.6414e+02,  ...,  6.9611e+01,\n",
      "            1.8371e+01,  2.7794e+00],\n",
      "          [-6.3975e+01, -7.2926e+01, -7.2238e+01,  ..., -2.2545e+01,\n",
      "           -4.9782e+00,  2.7794e+00],\n",
      "          [ 5.7600e+00,  1.4538e+00,  1.4538e+00,  ...,  1.4538e+00,\n",
      "            1.4538e+00,  4.9861e+00]],\n",
      "\n",
      "         [[-2.9579e+01, -3.1248e+01, -5.6205e+01,  ...,  1.7746e+01,\n",
      "           -3.4578e+01, -3.4715e+01],\n",
      "          [-2.6133e+01, -3.0694e+01, -4.7128e+01,  ...,  1.9715e+01,\n",
      "           -3.1372e+01, -3.0082e+01],\n",
      "          [-5.2569e+01, -1.8862e+01, -2.1007e+01,  ...,  1.6206e+01,\n",
      "           -3.0873e+01, -3.0082e+01],\n",
      "          ...,\n",
      "          [-6.2223e+01, -6.8996e+01, -6.6671e+01,  ..., -2.4523e+01,\n",
      "           -3.7440e+01, -3.0082e+01],\n",
      "          [-3.8325e+01, -3.9207e+01, -3.9569e+01,  ..., -3.1458e+01,\n",
      "           -3.1793e+01, -3.0082e+01],\n",
      "          [-2.9669e+01, -2.8642e+01, -2.8642e+01,  ..., -2.8642e+01,\n",
      "           -2.8642e+01, -3.0478e+01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.8299e+00,  1.5294e+01,  2.2479e-01,  ...,  9.8640e+00,\n",
      "           -3.0268e+00,  9.3306e+00],\n",
      "          [ 8.0173e+00,  1.6694e+01,  2.0626e+00,  ...,  2.0711e+01,\n",
      "            8.7881e+00,  2.0319e+01],\n",
      "          [-7.5066e+00,  9.0119e+00, -1.1106e+00,  ...,  1.8857e+01,\n",
      "            8.5433e+00,  2.0319e+01],\n",
      "          ...,\n",
      "          [ 2.7765e+01,  2.2128e+01,  1.8974e+01,  ..., -5.4140e+00,\n",
      "            6.0002e+00,  2.0319e+01],\n",
      "          [-8.7161e+00, -1.4767e+01, -1.5562e+01,  ...,  1.1602e+00,\n",
      "            6.7248e+00,  2.0319e+01],\n",
      "          [ 7.2518e+00,  8.9498e-01,  8.9498e-01,  ...,  8.9498e-01,\n",
      "            8.9498e-01,  1.5803e+01]],\n",
      "\n",
      "         [[-9.0524e+00, -3.2217e+00, -1.4809e+01,  ...,  8.5778e-01,\n",
      "           -6.1471e+00,  4.7238e+00],\n",
      "          [ 1.3498e+01,  8.6447e+00, -1.5793e+01,  ...,  2.5944e+00,\n",
      "           -4.7681e+00,  6.2281e+00],\n",
      "          [ 1.2303e+01,  8.4679e+00, -1.4526e+00,  ...,  9.3636e-01,\n",
      "           -4.6044e+00,  6.2281e+00],\n",
      "          ...,\n",
      "          [-1.3846e+01, -9.2846e+00, -8.1554e+00,  ..., -1.4215e+01,\n",
      "           -7.2449e+00,  6.2281e+00],\n",
      "          [-2.3425e+00, -5.2558e+00, -5.8610e+00,  ..., -9.5041e+00,\n",
      "           -4.3338e+00,  6.2281e+00],\n",
      "          [-1.2111e+01, -9.3915e+00, -9.3915e+00,  ..., -9.3915e+00,\n",
      "           -9.3915e+00,  1.3196e+00]],\n",
      "\n",
      "         [[ 8.9303e+00, -4.7483e-01,  1.1582e+01,  ..., -1.1239e+01,\n",
      "           -1.2163e+01, -9.9583e+00],\n",
      "          [-1.4871e+01, -9.5663e+00, -6.5747e-01,  ..., -1.4194e+01,\n",
      "           -8.1737e+00, -4.1303e+00],\n",
      "          [ 2.6385e+00, -8.8743e+00, -1.3194e+01,  ..., -1.0967e+01,\n",
      "           -8.8808e+00, -4.1303e+00],\n",
      "          ...,\n",
      "          [-3.2359e+01, -4.0792e+01, -3.2461e+01,  ...,  4.8772e+01,\n",
      "           -1.4831e-01, -4.1303e+00],\n",
      "          [-2.7120e+01, -2.7607e+01, -2.6676e+01,  ..., -1.1070e+01,\n",
      "           -1.7788e+01, -4.1303e+00],\n",
      "          [-4.2715e+00, -1.0045e+01, -1.0045e+01,  ..., -1.0045e+01,\n",
      "           -1.0045e+01, -3.8049e+00]]],\n",
      "\n",
      "\n",
      "        [[[-4.6701e+00,  4.0173e+00, -1.1016e+00,  ...,  4.3227e+00,\n",
      "            7.4036e-01,  6.0621e+00],\n",
      "          [-1.5089e+01, -6.7497e+00, -2.3541e+01,  ...,  2.8818e+00,\n",
      "           -1.6923e+00, -4.1354e-01],\n",
      "          [ 1.3169e+01, -7.7121e+00, -1.4140e+01,  ..., -2.0928e-01,\n",
      "           -5.7443e-01,  9.8211e-02],\n",
      "          ...,\n",
      "          [-2.2780e+00, -1.8652e+00,  4.3254e+00,  ..., -4.3268e+00,\n",
      "            4.0113e+00,  2.0143e+00],\n",
      "          [ 3.6357e+01,  4.5737e+01,  2.9971e+01,  ...,  3.9767e+01,\n",
      "            8.4800e+00,  5.5449e+00],\n",
      "          [-2.9857e+01, -3.5019e+01, -3.7746e+01,  ..., -1.3081e+01,\n",
      "           -2.0860e+01, -4.3789e+00]],\n",
      "\n",
      "         [[-3.1894e+01, -4.0666e+01, -4.4316e+01,  ..., -2.1628e+01,\n",
      "           -3.3083e+01, -3.0280e+01],\n",
      "          [-6.6440e+00,  5.8464e+00,  1.2671e+01,  ..., -5.8727e+00,\n",
      "            2.1974e+00,  8.7969e+00],\n",
      "          [-2.3051e+01, -1.9959e+01, -2.5741e+01,  ..., -5.1601e+00,\n",
      "            2.2527e+00,  8.6427e+00],\n",
      "          ...,\n",
      "          [-2.7298e+01, -2.3101e+01, -6.8450e+00,  ..., -5.7860e+00,\n",
      "            4.8569e+00,  9.0439e+00],\n",
      "          [-2.6913e+01, -3.0007e+01, -2.2974e+01,  ..., -4.0307e+00,\n",
      "           -4.6672e-01,  5.6865e+00],\n",
      "          [ 1.0372e+02,  1.0756e+02,  8.3625e+01,  ...,  6.5996e+01,\n",
      "            3.0708e+01,  1.2188e+01]],\n",
      "\n",
      "         [[-3.8443e+01, -6.9817e+01, -7.6199e+01,  ..., -9.6794e+00,\n",
      "            3.2902e+01, -3.8418e+01],\n",
      "          [-1.9918e+01, -4.9508e+01, -9.8411e+01,  ..., -2.5789e+00,\n",
      "            2.5913e+01, -3.5524e+01],\n",
      "          [-9.7323e+00, -4.6214e+01, -1.0950e+02,  ..., -1.2412e+00,\n",
      "            2.3605e+01, -3.5921e+01],\n",
      "          ...,\n",
      "          [-1.3776e+01,  1.2627e+01,  5.7059e+00,  ..., -1.0768e+01,\n",
      "            2.6664e+01, -3.4540e+01],\n",
      "          [-5.0712e+01, -9.7141e+00,  1.0220e+01,  ..., -1.3079e+01,\n",
      "            2.7298e+01, -2.7150e+01],\n",
      "          [-4.6437e+01, -4.0847e+01, -3.2514e+01,  ..., -3.8398e+01,\n",
      "           -2.9367e+01, -3.3866e+01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.4402e+00, -4.5897e-01, -1.7715e+00,  ..., -6.2468e+00,\n",
      "            1.2097e+01,  1.3009e+01],\n",
      "          [ 6.6648e+00,  1.7597e+01,  5.5529e+00,  ..., -4.7464e-01,\n",
      "            2.2705e+01,  2.3611e+01],\n",
      "          [ 1.1361e+01,  9.2768e+00,  1.8879e+00,  ..., -1.0823e+00,\n",
      "            2.2455e+01,  2.3491e+01],\n",
      "          ...,\n",
      "          [ 9.6447e+00,  1.8633e+01,  3.6291e+01,  ..., -9.4144e+00,\n",
      "            1.9545e+01,  2.3425e+01],\n",
      "          [ 1.1066e+01,  1.7404e+01,  3.2545e+01,  ...,  5.0491e+00,\n",
      "            1.3584e+01,  2.2138e+01],\n",
      "          [ 1.1049e+01,  4.4456e+00,  1.0734e+01,  ..., -1.2417e+00,\n",
      "           -1.8799e+00,  1.5471e+01]],\n",
      "\n",
      "         [[ 6.8354e+00, -7.6379e+00, -7.8200e+00,  ..., -1.6778e+01,\n",
      "            1.1680e+00,  4.5875e+00],\n",
      "          [ 1.2776e+01,  1.8622e+01,  1.8469e+01,  ..., -1.6694e+01,\n",
      "           -1.0278e+00,  4.2608e+00],\n",
      "          [-5.4903e+00,  2.1248e+01, -5.6635e+00,  ..., -1.4421e+01,\n",
      "           -1.2841e+00,  4.0656e+00],\n",
      "          ...,\n",
      "          [ 4.0521e+00,  1.1611e+01,  6.2973e+00,  ..., -2.6402e+01,\n",
      "           -1.5723e+01, -2.0662e+00],\n",
      "          [-7.6177e+01, -6.3563e+01, -2.9150e+01,  ..., -4.9311e+01,\n",
      "            3.1908e+00,  7.9010e+00],\n",
      "          [ 1.2835e+00,  6.5799e-01, -4.0487e+00,  ..., -4.7665e+00,\n",
      "           -1.5433e+01,  1.6882e+00]],\n",
      "\n",
      "         [[-5.7790e+00, -5.6192e+00,  6.6508e+00,  ..., -1.4709e+01,\n",
      "           -7.1542e-01, -7.5264e+00],\n",
      "          [-4.9799e+00, -5.6072e+00, -1.8627e+01,  ..., -9.6116e+00,\n",
      "           -7.9366e+00, -3.4191e-02],\n",
      "          [-5.6180e+00, -1.3116e+01, -1.0731e+01,  ..., -7.4793e+00,\n",
      "           -8.5504e+00,  4.3728e-01],\n",
      "          ...,\n",
      "          [ 6.3552e+00,  1.7820e+01,  9.7499e+00,  ...,  2.8062e+00,\n",
      "            3.2348e+00,  7.6028e+00],\n",
      "          [ 5.5187e+00,  1.5420e+01,  2.8882e+00,  ..., -6.4590e+00,\n",
      "           -1.8723e+01, -5.9322e+00],\n",
      "          [-1.3711e+01, -1.8332e+01,  5.1910e+00,  ..., -2.2537e+01,\n",
      "            2.0378e+01, -1.5560e+00]]],\n",
      "\n",
      "\n",
      "        [[[-2.7477e+01,  2.5761e+01,  4.4009e+01,  ...,  6.8147e+01,\n",
      "            5.6313e+01,  4.3132e+01],\n",
      "          [-7.5134e+00,  1.0604e+00,  1.5613e+01,  ..., -3.8121e+00,\n",
      "            1.7116e+01,  1.5442e+01],\n",
      "          [-1.3847e+01, -1.3967e+01,  1.4046e+00,  ..., -1.7859e+01,\n",
      "           -1.0819e+01,  4.1812e+00],\n",
      "          ...,\n",
      "          [-1.4523e+01, -3.9599e+00,  6.5182e+00,  ...,  1.4381e+00,\n",
      "           -1.7916e+00,  2.0975e+00],\n",
      "          [-1.7383e+01, -1.1606e+01,  1.8168e+00,  ...,  3.7349e+00,\n",
      "            1.1765e+00,  8.4318e+00],\n",
      "          [-2.4907e+01, -2.0385e+01, -7.9450e+00,  ..., -1.2612e+01,\n",
      "           -6.4935e+00, -5.8738e+00]],\n",
      "\n",
      "         [[-1.2428e+02, -1.5809e+02, -1.4867e+02,  ..., -1.5395e+02,\n",
      "           -1.5524e+02, -1.5862e+02],\n",
      "          [ 1.0711e+01,  2.5817e+01,  4.1905e+01,  ...,  8.4359e+01,\n",
      "            7.0922e+01,  6.6399e+01],\n",
      "          [-2.6739e+01, -3.1501e+01, -2.1966e+01,  ..., -3.2718e+00,\n",
      "           -6.8164e+00, -5.0933e+00],\n",
      "          ...,\n",
      "          [-2.2217e+01, -2.5570e+01, -1.5968e+01,  ..., -1.3527e+01,\n",
      "           -1.7729e+01, -2.5083e+01],\n",
      "          [-2.2000e+01, -2.3281e+01, -1.5227e+01,  ..., -2.3491e+01,\n",
      "           -2.4157e+01, -2.9353e+01],\n",
      "          [-3.9384e+01, -4.2055e+01, -2.3193e+01,  ..., -2.5602e+01,\n",
      "           -2.3051e+01, -2.2005e+01]],\n",
      "\n",
      "         [[-2.2573e+02, -4.3334e+01, -4.5724e+01,  ..., -4.7714e+01,\n",
      "           -6.1567e+01, -7.6603e+01],\n",
      "          [-2.4980e+02, -3.0463e+01, -3.4744e+01,  ..., -6.7751e+01,\n",
      "           -8.9720e+01, -8.6502e+01],\n",
      "          [-2.4828e+02, -2.5422e+01, -2.8232e+01,  ..., -1.0195e+02,\n",
      "           -7.3338e+01, -9.8083e+01],\n",
      "          ...,\n",
      "          [-2.1589e+02, -2.1700e+01, -2.4185e+01,  ..., -3.2232e+01,\n",
      "           -2.6404e+01, -2.9942e+01],\n",
      "          [-2.1012e+02, -1.8237e+01, -3.5099e+01,  ..., -2.8288e+01,\n",
      "           -3.0030e+01, -2.9746e+01],\n",
      "          [-2.1736e+02, -3.2337e+00, -3.3833e+01,  ..., -2.4380e+01,\n",
      "           -2.5261e+01, -2.2730e+01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.0104e+01, -2.0659e+01, -2.5463e+01,  ..., -7.5864e-01,\n",
      "           -1.2782e+01, -1.0355e+01],\n",
      "          [-3.4670e+01, -9.2727e-01, -4.1725e+00,  ...,  1.0181e+01,\n",
      "            1.0154e+01,  1.8138e+01],\n",
      "          [-4.2789e+01, -3.3992e+00, -2.0418e+00,  ...,  3.0254e+00,\n",
      "            7.0188e+00,  1.3880e+01],\n",
      "          ...,\n",
      "          [-3.7515e+01,  1.5636e+00,  1.8459e+00,  ..., -1.2396e+00,\n",
      "           -1.7671e+00,  3.3767e+00],\n",
      "          [-3.9080e+01, -1.7492e-01, -1.5584e-01,  ..., -5.9147e-01,\n",
      "           -1.7860e+00,  3.7696e+00],\n",
      "          [-3.7094e+01,  7.0878e+00,  1.1896e+01,  ...,  1.0655e+01,\n",
      "            1.3476e+01,  1.4655e+01]],\n",
      "\n",
      "         [[-3.8614e+01, -1.1065e+01, -1.8344e+01,  ..., -4.1984e+01,\n",
      "           -2.3464e+01, -4.1349e+01],\n",
      "          [-3.0098e+01, -8.3128e+00, -4.2960e+00,  ...,  2.6975e+00,\n",
      "           -2.1902e+01, -1.3797e+01],\n",
      "          [-2.7154e+01, -4.8671e+00,  4.7610e+00,  ..., -2.1218e+01,\n",
      "            9.2780e+00, -2.0910e+01],\n",
      "          ...,\n",
      "          [-2.9603e+01, -8.6954e+00,  5.9418e+00,  ...,  3.9975e+00,\n",
      "            8.9910e+00,  3.7629e+00],\n",
      "          [-2.3495e+01,  1.0469e+00,  3.7798e+00,  ...,  6.3023e+00,\n",
      "            1.2963e+00, -6.7114e+00],\n",
      "          [-2.1519e+01,  1.2733e+01,  6.9889e+00,  ...,  6.9628e+00,\n",
      "           -5.2086e-01,  2.4396e+00]],\n",
      "\n",
      "         [[ 5.6412e+01, -5.4859e+01, -1.7363e+01,  ..., -5.7017e-01,\n",
      "           -1.1202e+01, -4.4573e+01],\n",
      "          [-2.2015e+01, -5.2699e+01, -2.7219e+01,  ..., -1.7801e+01,\n",
      "           -6.4549e+00, -1.9914e+01],\n",
      "          [ 7.2230e+00, -1.6757e+01,  6.7049e-01,  ...,  2.9255e+01,\n",
      "           -9.0490e+00,  9.0586e+00],\n",
      "          ...,\n",
      "          [ 3.7723e+00, -2.0882e+01, -4.2384e-01,  ...,  1.4142e-01,\n",
      "           -4.0668e+00, -3.5411e+00],\n",
      "          [ 1.2160e+00, -2.2452e+01,  1.3286e+00,  ..., -7.9563e+00,\n",
      "           -2.7022e+00, -1.3566e+00],\n",
      "          [-6.5169e+00, -3.5174e+01, -1.2150e+00,  ..., -4.0583e-01,\n",
      "            1.8555e+00, -1.2966e+00]]]], device='cuda:0',\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#### input floating number / weight quantized version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 128, out_channels=16, kernel_size = 3, bias = False)\n",
    "conv_ref.weight = model.layer1[0].conv1.weight_q\n",
    "\n",
    "output_ref = conv_ref(x)\n",
    "print(output_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "157dffd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "157dffd8",
    "outputId": "f69011ed-79e2-4eb3-d32f-c84f6c5375d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8496, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "difference = abs( output_ref - output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sorted-niger",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sorted-niger",
    "outputId": "ad7a835a-d532-4da0-e0d7-d701df9ba41e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.7184e+01,  3.7555e+01, -2.2583e+01,  ...,  4.0763e+01,\n",
      "            5.3191e+01,  2.3428e+01],\n",
      "          [ 2.2707e+00,  1.6311e+01, -8.7775e+00,  ...,  1.3177e+00,\n",
      "           -1.4327e+01, -1.2056e+01],\n",
      "          [ 2.2861e+00,  1.3801e+01, -1.3751e+01,  ..., -1.3010e+00,\n",
      "           -1.1512e+01,  2.3222e+00],\n",
      "          ...,\n",
      "          [ 2.3074e+00,  1.3784e+01, -1.3938e+01,  ...,  1.8514e+00,\n",
      "            6.1460e+00,  1.4700e+01],\n",
      "          [ 2.3074e+00,  1.3784e+01, -1.3938e+01,  ..., -1.6961e+00,\n",
      "            6.1797e+00,  1.5889e+01],\n",
      "          [ 9.7444e-02,  1.3997e+01, -2.6113e+01,  ..., -3.1117e+00,\n",
      "            5.6292e+00,  1.4374e+01]],\n",
      "\n",
      "         [[-4.2037e+01, -5.0824e+01, -1.2512e+02,  ..., -1.0133e+02,\n",
      "           -9.1581e+01, -9.1588e+01],\n",
      "          [ 4.4694e+00,  3.8135e+00,  1.8445e+01,  ...,  6.3256e+01,\n",
      "            4.5490e+01,  4.6940e+01],\n",
      "          [ 2.3021e+00, -1.5956e+01, -2.3398e+01,  ..., -7.0362e+00,\n",
      "           -1.4787e+01, -2.2996e+01],\n",
      "          ...,\n",
      "          [ 2.3840e+00, -1.6029e+01, -2.3992e+01,  ..., -8.9109e+00,\n",
      "           -8.1957e+00, -1.7435e+01],\n",
      "          [ 2.3840e+00, -1.6029e+01, -2.3992e+01,  ..., -1.0202e+01,\n",
      "           -9.3871e+00, -1.6976e+01],\n",
      "          [ 5.4891e+00, -1.3659e+01, -3.6216e+01,  ..., -1.5105e+01,\n",
      "           -1.3966e+01, -2.1142e+01]],\n",
      "\n",
      "         [[-4.0027e+01, -1.5534e+02, -2.5678e+02,  ..., -5.6132e+01,\n",
      "           -2.4156e+01, -1.5117e+02],\n",
      "          [-4.3562e+01, -1.6192e+02, -2.8447e+02,  ..., -5.6415e+01,\n",
      "           -6.2365e+01, -1.6726e+02],\n",
      "          [-4.3087e+01, -1.6110e+02, -2.8546e+02,  ..., -5.2337e+01,\n",
      "           -8.3831e+01, -1.7403e+02],\n",
      "          ...,\n",
      "          [-4.3073e+01, -1.6111e+02, -2.8551e+02,  ..., -3.6605e+01,\n",
      "           -4.2842e+01, -1.1205e+02],\n",
      "          [-4.3073e+01, -1.6111e+02, -2.8551e+02,  ..., -4.4856e+01,\n",
      "           -4.0578e+01, -1.1137e+02],\n",
      "          [-4.1193e+01, -1.5955e+02, -2.9055e+02,  ..., -5.6866e+01,\n",
      "           -3.9526e+01, -1.1136e+02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8859e+01,  3.4037e+01, -6.7514e+01,  ..., -1.2602e+01,\n",
      "            1.1216e+01,  7.0793e+00],\n",
      "          [ 3.8624e+01,  6.4630e+01, -3.4360e+01,  ...,  2.4128e+01,\n",
      "            2.9591e+01,  3.9476e+01],\n",
      "          [ 4.1578e+01,  6.2262e+01, -4.5455e+01,  ...,  1.2010e+01,\n",
      "            2.5650e+01,  2.4599e+01],\n",
      "          ...,\n",
      "          [ 4.1677e+01,  6.2287e+01, -4.5824e+01,  ...,  5.4486e+00,\n",
      "            1.9557e+01,  2.7660e+01],\n",
      "          [ 4.1677e+01,  6.2287e+01, -4.5824e+01,  ...,  2.5117e+00,\n",
      "            1.9139e+01,  2.7915e+01],\n",
      "          [ 3.8363e+01,  6.2945e+01, -3.9399e+01,  ..., -3.1870e+00,\n",
      "            1.5956e+01,  3.4673e+01]],\n",
      "\n",
      "         [[ 2.9196e+01,  4.2539e+01, -4.7482e+01,  ..., -3.1400e+01,\n",
      "            2.3481e+01, -9.3922e+00],\n",
      "          [ 1.9391e+01,  3.6038e+01, -3.7528e+01,  ...,  1.1678e+00,\n",
      "            2.6070e+01,  1.1400e+01],\n",
      "          [ 1.9905e+01,  3.5480e+01, -4.0654e+01,  ...,  1.5319e+01,\n",
      "            1.9364e+01, -1.0888e+01],\n",
      "          ...,\n",
      "          [ 1.9995e+01,  3.5577e+01, -4.0757e+01,  ...,  1.1347e+01,\n",
      "            1.6030e+01,  4.6093e+00],\n",
      "          [ 1.9995e+01,  3.5577e+01, -4.0757e+01,  ...,  6.8345e+00,\n",
      "            1.7385e+01,  3.9364e+00],\n",
      "          [ 1.3801e+01,  2.9993e+01, -3.3986e+01,  ..., -1.1550e-01,\n",
      "            1.1465e+01,  6.0650e+00]],\n",
      "\n",
      "         [[ 1.4233e+01, -3.7702e+01,  5.7148e+01,  ...,  2.4982e+01,\n",
      "           -4.8236e+00, -5.4211e+01],\n",
      "          [ 7.7207e+00, -2.7838e+00, -3.4983e+01,  ..., -1.5004e+01,\n",
      "           -3.3399e+00, -1.7783e+01],\n",
      "          [ 6.1312e+00,  1.4769e+01,  2.1437e-01,  ..., -1.0266e+01,\n",
      "            1.4473e+01,  1.8738e+00],\n",
      "          ...,\n",
      "          [ 6.0515e+00,  1.4744e+01,  6.3779e-01,  ..., -7.6418e+00,\n",
      "            7.2664e+00,  1.1498e+01],\n",
      "          [ 6.0515e+00,  1.4744e+01,  6.3779e-01,  ..., -9.7595e+00,\n",
      "            5.9823e+00,  1.0614e+01],\n",
      "          [ 1.0466e+01,  3.2480e+00, -1.2340e+01,  ..., -1.5935e+01,\n",
      "            3.4883e+00, -2.1701e-01]]],\n",
      "\n",
      "\n",
      "        [[[-3.0709e-01, -2.9466e+01,  8.2252e+00,  ...,  1.0470e+01,\n",
      "            1.4703e+01,  8.6734e+00],\n",
      "          [ 3.7450e+00, -2.4178e+01,  1.1795e+00,  ...,  1.4233e+01,\n",
      "            1.7895e+01,  1.3289e+01],\n",
      "          [-4.2209e+00, -2.4377e+00,  7.0479e+00,  ...,  5.6166e+00,\n",
      "            1.7388e+01,  1.4667e+01],\n",
      "          ...,\n",
      "          [-1.0287e+01, -6.9836e-01, -2.0828e+01,  ..., -2.1757e+01,\n",
      "           -2.9804e+01, -2.5464e+01],\n",
      "          [-1.2591e+00,  3.3874e-01,  4.6614e+00,  ...,  3.2852e+00,\n",
      "            3.9810e+00,  3.3226e+00],\n",
      "          [ 1.7484e+00,  2.4320e+00,  2.4320e+00,  ...,  2.4320e+00,\n",
      "            2.4320e+00,  1.0777e+00]],\n",
      "\n",
      "         [[-2.5776e+01, -4.0348e+01, -1.5669e+01,  ..., -1.8449e+01,\n",
      "           -1.8865e+01, -2.0594e+01],\n",
      "          [ 5.8659e+00, -1.8142e+01, -3.1580e+01,  ..., -1.4459e+01,\n",
      "           -2.1297e+01, -2.1469e+01],\n",
      "          [ 3.8004e+00, -1.4545e+01, -3.1408e+01,  ..., -6.3082e+00,\n",
      "           -7.1895e+00, -1.8246e+01],\n",
      "          ...,\n",
      "          [ 4.2929e+00,  3.6583e+01,  7.2837e+01,  ...,  6.9450e+01,\n",
      "            7.2746e+01,  7.6376e+01],\n",
      "          [ 4.4592e+00, -2.0944e+01, -3.4797e+01,  ..., -3.2226e+01,\n",
      "           -3.7675e+01, -2.9960e+01],\n",
      "          [ 6.0959e+00,  5.1651e-01,  5.1651e-01,  ...,  5.1651e-01,\n",
      "            5.1651e-01,  2.8686e+00]],\n",
      "\n",
      "         [[-1.0347e+02, -1.6896e+02, -3.3779e+01,  ..., -4.0340e+01,\n",
      "           -1.4188e+01, -1.4169e+00],\n",
      "          [-1.1030e+02, -1.8771e+02, -1.3343e+01,  ..., -5.5184e+01,\n",
      "           -2.1945e+01,  7.2430e-01],\n",
      "          [-1.1211e+02, -2.0406e+02, -1.9179e+01,  ..., -4.5566e+01,\n",
      "           -3.6662e+01, -1.9478e+01],\n",
      "          ...,\n",
      "          [-3.6945e+01, -4.0551e+01, -4.3877e+01,  ..., -4.3418e+01,\n",
      "           -3.8850e+01, -3.9454e+01],\n",
      "          [-3.2708e+01, -3.0431e+01, -3.0028e+01,  ..., -3.0232e+01,\n",
      "           -3.0171e+01, -3.1992e+01],\n",
      "          [-3.1209e+01, -3.0252e+01, -3.0252e+01,  ..., -3.0252e+01,\n",
      "           -3.0252e+01, -3.2230e+01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.7130e+01, -2.4476e+01,  1.0246e+01,  ...,  4.6485e+00,\n",
      "            9.6526e+00,  1.0782e+01],\n",
      "          [ 5.5818e+01, -2.9255e+01, -8.0981e-01,  ...,  3.4509e+00,\n",
      "            6.6638e+00,  9.9568e+00],\n",
      "          [ 4.9113e+01, -2.2322e+01,  1.5864e+00,  ...,  3.3711e+00,\n",
      "            8.9993e+00,  5.0684e+00],\n",
      "          ...,\n",
      "          [ 2.8998e+01,  2.4417e+01,  1.7986e+01,  ...,  1.5977e+01,\n",
      "            9.8387e+00,  1.4799e+01],\n",
      "          [ 1.3974e+01, -3.2964e+00, -2.3100e+00,  ..., -1.6296e+00,\n",
      "           -2.5111e+00,  1.3311e+01],\n",
      "          [ 9.3356e+00,  1.6251e+00,  1.6251e+00,  ...,  1.6251e+00,\n",
      "            1.6251e+00,  1.6651e+01]],\n",
      "\n",
      "         [[ 1.5416e+01, -2.0107e+01,  3.6866e+00,  ...,  8.0165e+00,\n",
      "            1.3485e+01,  1.7923e+01],\n",
      "          [ 2.7152e+01, -1.5720e+01,  5.4486e+00,  ..., -7.9592e+00,\n",
      "            1.1051e+01,  1.9781e+01],\n",
      "          [ 2.5878e+01, -4.8558e+01, -4.8999e+00,  ...,  1.1707e+00,\n",
      "           -3.4699e+00,  9.2703e+00],\n",
      "          ...,\n",
      "          [-8.8876e+00,  1.6052e+01,  1.0260e+01,  ...,  9.8179e+00,\n",
      "            6.6655e+00,  1.6998e+01],\n",
      "          [ 5.0131e+00,  1.9492e+00,  1.7845e+00,  ...,  9.3889e-01,\n",
      "           -1.4959e+00,  8.2108e+00],\n",
      "          [-9.2864e+00, -7.7465e+00, -7.7465e+00,  ..., -7.7465e+00,\n",
      "           -7.7465e+00,  3.1262e+00]],\n",
      "\n",
      "         [[-1.9526e+01,  8.8609e+00, -5.1647e+01,  ..., -1.2226e+01,\n",
      "           -1.7640e+01, -4.2516e+00],\n",
      "          [-1.6219e+01, -1.3732e+01, -2.3330e+01,  ...,  5.3705e-01,\n",
      "           -1.7361e+01, -3.5237e-01],\n",
      "          [-7.7697e+00, -1.4453e+00, -2.0554e+01,  ..., -1.0334e+01,\n",
      "           -1.0552e+00, -3.2035e+00],\n",
      "          ...,\n",
      "          [ 7.0379e+00, -4.9648e+01, -4.3933e+01,  ..., -4.0953e+01,\n",
      "           -1.0888e+01, -1.6178e+01],\n",
      "          [-1.5427e+01, -4.2479e+00, -8.4675e+00,  ..., -9.8486e+00,\n",
      "           -1.0496e+01, -2.6006e+00],\n",
      "          [-3.8650e+00, -9.9141e+00, -9.9141e+00,  ..., -9.9141e+00,\n",
      "           -9.9141e+00, -2.5536e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7304e+01, -1.5374e+00, -2.0078e+01,  ..., -6.9509e+00,\n",
      "           -6.1630e+00,  2.1933e+00],\n",
      "          [-3.4727e+00,  4.3612e+00,  1.3822e+01,  ..., -8.1937e+00,\n",
      "           -5.3119e+00, -3.1886e+00],\n",
      "          [-2.4679e+01, -1.8713e+01, -4.8871e+00,  ..., -8.9115e+00,\n",
      "           -5.0578e+00, -3.3062e+00],\n",
      "          ...,\n",
      "          [ 4.3559e+00,  6.8359e+00,  1.9657e+01,  ...,  2.9316e+01,\n",
      "            2.1088e+01,  1.6993e-01],\n",
      "          [ 1.1489e+01, -1.4205e+01, -1.1922e+01,  ...,  1.0733e+01,\n",
      "            2.1830e+01, -4.6562e-01],\n",
      "          [-1.3838e+01, -2.4397e+01, -1.4113e+01,  ..., -2.0679e-02,\n",
      "            1.8548e+01, -1.7146e-01]],\n",
      "\n",
      "         [[-4.8883e+01, -5.6799e+01, -3.0134e+01,  ..., -1.3229e+01,\n",
      "           -3.4988e+01, -2.9949e+01],\n",
      "          [ 2.8713e+01,  2.4173e+00, -3.3964e+01,  ..., -2.0068e+01,\n",
      "           -1.1952e+01,  6.5576e+00],\n",
      "          [-1.2794e+01,  2.7748e+01,  4.6136e+01,  ..., -2.0147e+01,\n",
      "           -1.1991e+01,  6.5106e+00],\n",
      "          ...,\n",
      "          [-1.4352e+01, -2.4651e+01, -1.3972e+01,  ..., -3.9272e+01,\n",
      "           -1.0780e+01,  8.5403e+00],\n",
      "          [ 1.7667e+01,  1.8303e+01,  9.4213e+00,  ..., -3.8243e+01,\n",
      "           -1.1535e+01,  9.4994e+00],\n",
      "          [-2.7400e+00, -1.9000e+01, -1.7897e+01,  ..., -3.3638e+01,\n",
      "           -1.8367e+01,  1.0243e+01]],\n",
      "\n",
      "         [[-4.2218e+01, -4.2910e+00,  1.9664e+01,  ..., -1.0313e+01,\n",
      "            5.6848e+01, -3.6800e+01],\n",
      "          [-5.5720e+01, -5.9880e+01, -2.0050e+01,  ..., -1.2715e+01,\n",
      "            4.9792e+01, -3.6428e+01],\n",
      "          [ 1.4445e+00, -2.2510e+01, -4.2800e+01,  ..., -1.2790e+01,\n",
      "            4.9361e+01, -3.6555e+01],\n",
      "          ...,\n",
      "          [-8.8661e+00,  6.7263e+00, -5.4322e-01,  ..., -8.0829e+00,\n",
      "            5.0671e+01, -2.9962e+01],\n",
      "          [ 8.8487e+00, -1.0087e+01, -1.3151e+01,  ..., -1.9335e+01,\n",
      "            5.9632e+01, -2.9364e+01],\n",
      "          [-1.4308e+01, -4.5119e+01, -5.1938e+00,  ..., -6.6331e+01,\n",
      "            8.5172e+01, -2.9392e+01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.4974e+00, -7.2559e+00, -2.9790e+00,  ..., -1.8126e+01,\n",
      "            1.1390e+01,  1.6874e+01],\n",
      "          [ 1.0177e+01, -4.3313e-02, -3.2328e+00,  ..., -1.2922e+01,\n",
      "            2.6880e+01,  2.7427e+01],\n",
      "          [ 9.5990e+00,  1.4395e+01,  1.8404e+01,  ..., -1.2315e+01,\n",
      "            2.7232e+01,  2.7325e+01],\n",
      "          ...,\n",
      "          [ 1.3875e+01,  1.6884e+01,  2.2793e+01,  ..., -2.7989e+00,\n",
      "            4.3991e+01,  3.0545e+01],\n",
      "          [ 3.5732e+01,  1.7289e+01,  1.4009e+01,  ..., -2.1600e+01,\n",
      "            4.2191e+01,  3.0731e+01],\n",
      "          [ 2.2562e+01,  2.2096e-01,  1.3353e+01,  ..., -4.9935e+01,\n",
      "            2.9247e+01,  2.8157e+01]],\n",
      "\n",
      "         [[-1.2919e+01,  1.4200e+01,  3.6398e+01,  ..., -4.6988e+00,\n",
      "            1.6233e+01,  1.3230e+01],\n",
      "          [-1.2808e+01, -3.3229e+01, -3.5431e+01,  ..., -1.1097e+01,\n",
      "            1.6859e+01,  1.3024e+01],\n",
      "          [ 3.4709e+01,  1.0820e+01, -1.8320e+01,  ..., -9.7486e+00,\n",
      "            1.7058e+01,  1.2996e+01],\n",
      "          ...,\n",
      "          [-3.2251e+01, -1.3437e+01,  1.5361e+01,  ...,  1.1247e+01,\n",
      "            2.5150e+01,  1.2316e+01],\n",
      "          [ 1.9232e+00,  2.3421e+01,  1.9200e+01,  ...,  1.0354e+01,\n",
      "            2.3255e+01,  1.1618e+01],\n",
      "          [ 3.0706e+01,  2.1113e+01,  2.2497e+01,  ..., -2.1811e+01,\n",
      "            2.7541e+01,  1.3116e+00]],\n",
      "\n",
      "         [[-6.1705e+00, -2.3141e+01, -2.7626e+01,  ..., -3.5474e+01,\n",
      "           -1.2825e+01, -9.1213e+00],\n",
      "          [ 5.1614e+00,  2.2580e+00, -7.9598e-01,  ..., -2.7697e+01,\n",
      "           -3.3133e+01, -5.9927e+00],\n",
      "          [-4.1761e+01, -1.7447e+01, -2.6708e+01,  ..., -2.8541e+01,\n",
      "           -3.3281e+01, -5.8391e+00],\n",
      "          ...,\n",
      "          [-1.1202e+01,  9.3245e+00, -2.1137e+01,  ...,  7.2909e+00,\n",
      "           -6.0108e+00, -5.0446e+00],\n",
      "          [-2.8541e+01, -1.4659e+01, -4.4478e-01,  ..., -3.9976e+00,\n",
      "           -2.5815e+00, -3.9644e+00],\n",
      "          [-1.5343e+01, -6.2937e-02, -4.1558e+00,  ...,  1.4598e+01,\n",
      "           -1.2585e+01,  5.4950e+00]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 9.3142e+00,  5.7052e+00,  4.6344e+00,  ...,  1.0975e+01,\n",
      "            1.0224e+01,  8.6314e+00],\n",
      "          [-3.0400e+00, -9.9220e+00, -3.7956e+00,  ...,  2.7620e+00,\n",
      "            1.2777e+00,  1.5143e+00],\n",
      "          [-1.6844e+01, -4.6057e+00, -1.7935e+00,  ...,  1.8303e-01,\n",
      "            9.5876e-01,  1.5143e+00],\n",
      "          ...,\n",
      "          [-5.4233e+01, -6.2386e+01, -6.5270e+01,  ..., -4.8840e+01,\n",
      "           -1.3094e+01,  1.5143e+00],\n",
      "          [ 1.3785e+01,  1.6385e+01,  1.5493e+01,  ..., -1.2796e-01,\n",
      "            1.2523e+00,  1.5143e+00],\n",
      "          [ 1.7484e+00,  2.4320e+00,  2.4320e+00,  ...,  2.4320e+00,\n",
      "            2.4320e+00,  1.0777e+00]],\n",
      "\n",
      "         [[-1.7778e+01, -3.3737e+01, -4.5437e+01,  ..., -4.0897e+01,\n",
      "           -4.3371e+01, -3.8862e+01],\n",
      "          [ 7.9804e+00,  8.3486e+00,  8.8813e+00,  ...,  6.7755e-01,\n",
      "            4.6211e+00,  1.0112e+00],\n",
      "          [-8.5254e+00, -1.9606e+01, -8.6030e+00,  ...,  8.9484e-01,\n",
      "            4.6064e+00,  1.0112e+00],\n",
      "          ...,\n",
      "          [ 1.3526e+02,  1.5957e+02,  1.6488e+02,  ...,  6.9746e+01,\n",
      "            1.7899e+01,  1.0112e+00],\n",
      "          [-6.3124e+01, -7.3500e+01, -7.2597e+01,  ..., -2.1707e+01,\n",
      "           -5.0830e+00,  1.0112e+00],\n",
      "          [ 6.0959e+00,  5.1651e-01,  5.1651e-01,  ...,  5.1651e-01,\n",
      "            5.1651e-01,  2.8686e+00]],\n",
      "\n",
      "         [[-3.0643e+01, -3.4059e+01, -5.9750e+01,  ...,  1.7153e+01,\n",
      "           -3.3949e+01, -3.5429e+01],\n",
      "          [-2.8190e+01, -3.4765e+01, -5.0113e+01,  ...,  1.6845e+01,\n",
      "           -3.3351e+01, -3.2673e+01],\n",
      "          [-5.4166e+01, -2.2582e+01, -2.3206e+01,  ...,  1.3296e+01,\n",
      "           -3.2777e+01, -3.2673e+01],\n",
      "          ...,\n",
      "          [-6.5005e+01, -6.9468e+01, -6.6700e+01,  ..., -2.6092e+01,\n",
      "           -3.7896e+01, -3.2673e+01],\n",
      "          [-3.5485e+01, -3.6859e+01, -3.7364e+01,  ..., -3.4194e+01,\n",
      "           -3.4456e+01, -3.2673e+01],\n",
      "          [-3.1209e+01, -3.0252e+01, -3.0252e+01,  ..., -3.0252e+01,\n",
      "           -3.0252e+01, -3.2230e+01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.9994e+00,  1.7292e+01,  3.1459e+00,  ...,  1.1742e+01,\n",
      "           -9.6010e-02,  1.1993e+01],\n",
      "          [ 1.0109e+01,  1.9910e+01,  4.3505e+00,  ...,  2.2400e+01,\n",
      "            1.0735e+01,  2.1446e+01],\n",
      "          [-5.6621e+00,  1.1896e+01,  1.1827e+00,  ...,  2.0587e+01,\n",
      "            1.0511e+01,  2.1446e+01],\n",
      "          ...,\n",
      "          [ 3.0696e+01,  2.5042e+01,  2.1237e+01,  ..., -6.0654e+00,\n",
      "            8.9576e+00,  2.1446e+01],\n",
      "          [-5.0921e+00, -1.0881e+01, -1.1447e+01,  ...,  4.9424e+00,\n",
      "            9.3189e+00,  2.1446e+01],\n",
      "          [ 9.3356e+00,  1.6251e+00,  1.6251e+00,  ...,  1.6251e+00,\n",
      "            1.6251e+00,  1.6651e+01]],\n",
      "\n",
      "         [[-4.9269e+00,  1.7802e+00, -9.7103e+00,  ...,  6.3759e+00,\n",
      "           -1.0121e+00,  9.9376e+00],\n",
      "          [ 1.7360e+01,  1.2564e+01, -1.2712e+01,  ...,  7.1827e+00,\n",
      "           -9.4537e-01,  9.9842e+00],\n",
      "          [ 1.6318e+01,  1.3050e+01,  2.6433e+00,  ...,  5.6086e+00,\n",
      "           -7.8514e-01,  9.9842e+00],\n",
      "          ...,\n",
      "          [-8.2668e+00, -5.7153e+00, -5.2112e+00,  ..., -1.1899e+01,\n",
      "           -3.7833e+00,  9.9842e+00],\n",
      "          [ 3.4756e+00,  1.8067e-01, -4.1377e-01,  ..., -4.4673e+00,\n",
      "           -1.4748e-01,  9.9842e+00],\n",
      "          [-9.2864e+00, -7.7465e+00, -7.7465e+00,  ..., -7.7465e+00,\n",
      "           -7.7465e+00,  3.1262e+00]],\n",
      "\n",
      "         [[ 8.7982e+00, -5.8913e-01,  1.0301e+01,  ..., -1.1059e+01,\n",
      "           -1.1914e+01, -8.5320e+00],\n",
      "          [-1.4957e+01, -1.0242e+01, -1.1531e+00,  ..., -1.4559e+01,\n",
      "           -8.3742e+00, -3.5282e+00],\n",
      "          [ 1.8456e+00, -9.5708e+00, -1.4043e+01,  ..., -1.1395e+01,\n",
      "           -9.0678e+00, -3.5282e+00],\n",
      "          ...,\n",
      "          [-3.2681e+01, -4.1152e+01, -3.2796e+01,  ...,  4.7457e+01,\n",
      "           -9.6656e-01, -3.5282e+00],\n",
      "          [-2.5963e+01, -2.6811e+01, -2.5818e+01,  ..., -1.1950e+01,\n",
      "           -1.8854e+01, -3.5282e+00],\n",
      "          [-3.8650e+00, -9.9141e+00, -9.9141e+00,  ..., -9.9141e+00,\n",
      "           -9.9141e+00, -2.5536e+00]]],\n",
      "\n",
      "\n",
      "        [[[-3.7561e+00,  6.1060e+00,  2.4914e+00,  ...,  6.7527e+00,\n",
      "            3.4778e+00,  7.3174e+00],\n",
      "          [-1.4368e+01, -6.1081e+00, -2.0482e+01,  ...,  4.5075e+00,\n",
      "           -5.7471e-02,  1.0183e+00],\n",
      "          [ 1.4553e+01, -4.8133e+00, -1.0501e+01,  ...,  1.7006e+00,\n",
      "            9.9688e-01,  1.4948e+00],\n",
      "          ...,\n",
      "          [ 1.5768e+00,  5.5980e-01,  5.2525e+00,  ..., -2.0147e+00,\n",
      "            5.1101e+00,  3.0505e+00],\n",
      "          [ 3.9971e+01,  4.8938e+01,  3.2641e+01,  ...,  4.2605e+01,\n",
      "            1.1402e+01,  7.0076e+00],\n",
      "          [-3.0693e+01, -3.4011e+01, -3.6062e+01,  ..., -1.1920e+01,\n",
      "           -1.8313e+01, -2.1421e+00]],\n",
      "\n",
      "         [[-3.1323e+01, -4.1130e+01, -4.5621e+01,  ..., -2.1066e+01,\n",
      "           -3.3643e+01, -3.0014e+01],\n",
      "          [-6.9886e+00,  4.0009e+00,  1.0757e+01,  ..., -5.0305e+00,\n",
      "            1.9093e+00,  8.3245e+00],\n",
      "          [-2.3652e+01, -2.0996e+01, -2.6621e+01,  ..., -4.2940e+00,\n",
      "            1.9713e+00,  8.0882e+00],\n",
      "          ...,\n",
      "          [-2.8468e+01, -2.4073e+01, -7.6269e+00,  ..., -6.4997e+00,\n",
      "            4.0229e+00,  8.2081e+00],\n",
      "          [-2.5962e+01, -2.9779e+01, -2.2680e+01,  ..., -3.8230e+00,\n",
      "           -2.3294e-01,  4.8522e+00],\n",
      "          [ 1.0556e+02,  1.0772e+02,  8.2966e+01,  ...,  6.5534e+01,\n",
      "            2.9357e+01,  1.0274e+01]],\n",
      "\n",
      "         [[-3.8285e+01, -6.9882e+01, -7.6930e+01,  ..., -9.7159e+00,\n",
      "            3.2048e+01, -3.8278e+01],\n",
      "          [-2.1519e+01, -5.1558e+01, -9.9550e+01,  ..., -4.1792e+00,\n",
      "            2.2697e+01, -3.7378e+01],\n",
      "          [-1.1258e+01, -4.8171e+01, -1.1185e+02,  ..., -2.7198e+00,\n",
      "            2.0590e+01, -3.7759e+01],\n",
      "          ...,\n",
      "          [-1.8517e+01,  9.1145e+00,  2.0414e+00,  ..., -1.1884e+01,\n",
      "            2.3884e+01, -3.6600e+01],\n",
      "          [-5.4583e+01, -1.2437e+01,  7.1412e+00,  ..., -1.4633e+01,\n",
      "            2.4763e+01, -2.8725e+01],\n",
      "          [-4.7223e+01, -3.9755e+01, -3.2079e+01,  ..., -3.7100e+01,\n",
      "           -2.9593e+01, -3.4554e+01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.0651e+00,  1.7000e+00, -3.7540e-01,  ..., -4.4304e+00,\n",
      "            1.3209e+01,  1.5971e+01],\n",
      "          [ 9.7915e+00,  2.0165e+01,  9.2831e+00,  ...,  1.6439e+00,\n",
      "            2.4547e+01,  2.5674e+01],\n",
      "          [ 1.2982e+01,  1.2580e+01,  6.6598e+00,  ...,  7.3598e-01,\n",
      "            2.4247e+01,  2.5496e+01],\n",
      "          ...,\n",
      "          [ 1.2351e+01,  2.1218e+01,  3.9074e+01,  ..., -6.7555e+00,\n",
      "            2.0887e+01,  2.4956e+01],\n",
      "          [ 1.0971e+01,  1.5909e+01,  3.2010e+01,  ...,  5.4764e+00,\n",
      "            1.4107e+01,  2.3696e+01],\n",
      "          [ 1.2541e+01,  5.6761e+00,  1.1713e+01,  ..., -5.3596e-02,\n",
      "           -1.1204e+00,  1.7066e+01]],\n",
      "\n",
      "         [[ 1.1365e+01, -3.7460e+00, -4.5186e+00,  ..., -1.2410e+01,\n",
      "            6.7558e+00,  1.0038e+01],\n",
      "          [ 1.7795e+01,  2.2007e+01,  2.0937e+01,  ..., -1.1756e+01,\n",
      "            4.1992e+00,  8.5942e+00],\n",
      "          [ 2.6700e-01,  2.6064e+01, -4.2810e-02,  ..., -9.6268e+00,\n",
      "            3.7990e+00,  8.3554e+00],\n",
      "          ...,\n",
      "          [ 9.3223e+00,  1.8350e+01,  1.3302e+01,  ..., -2.1056e+01,\n",
      "           -1.0212e+01,  2.2930e+00],\n",
      "          [-7.1665e+01, -5.9774e+01, -2.4503e+01,  ..., -4.5915e+01,\n",
      "            7.2578e+00,  1.1658e+01],\n",
      "          [ 2.6311e+00,  3.5830e-01, -4.1003e+00,  ..., -5.2536e+00,\n",
      "           -1.4539e+01,  3.3138e+00]],\n",
      "\n",
      "         [[-3.6284e+00, -4.2489e+00,  7.3733e+00,  ..., -1.6681e+01,\n",
      "           -5.2345e-01, -5.6784e+00],\n",
      "          [-3.4594e+00, -3.3013e+00, -1.7706e+01,  ..., -1.1652e+01,\n",
      "           -8.5135e+00,  7.5280e-01],\n",
      "          [-3.9875e+00, -1.1544e+01, -1.1297e+01,  ..., -9.4545e+00,\n",
      "           -9.0371e+00,  1.2459e+00],\n",
      "          ...,\n",
      "          [ 4.0630e+00,  1.6357e+01,  9.1162e+00,  ...,  1.0930e+00,\n",
      "            3.2877e+00,  8.7625e+00],\n",
      "          [ 4.3885e+00,  1.3027e+01,  1.6120e+00,  ..., -8.0881e+00,\n",
      "           -1.9144e+01, -5.2812e+00],\n",
      "          [-1.3139e+01, -1.8231e+01,  5.1981e+00,  ..., -2.3160e+01,\n",
      "            2.0166e+01, -8.4482e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.9678e+01,  3.3072e+01,  4.7175e+01,  ...,  6.9786e+01,\n",
      "            5.9620e+01,  4.8827e+01],\n",
      "          [-4.8382e+00,  8.3604e+00,  1.6816e+01,  ..., -3.2041e+00,\n",
      "            1.7322e+01,  1.8254e+01],\n",
      "          [-1.0934e+01, -5.3264e+00,  3.6198e+00,  ..., -1.3874e+01,\n",
      "           -7.6353e+00,  7.0338e+00],\n",
      "          ...,\n",
      "          [-1.1870e+01,  3.5224e+00,  8.6732e+00,  ...,  3.3959e+00,\n",
      "            8.3473e-01,  5.1312e+00],\n",
      "          [-1.4594e+01, -4.0177e+00,  4.0873e+00,  ...,  5.8872e+00,\n",
      "            3.6887e+00,  1.1220e+01],\n",
      "          [-2.1146e+01, -1.1904e+01, -5.5814e+00,  ..., -1.0140e+01,\n",
      "           -4.3149e+00, -3.0259e+00]],\n",
      "\n",
      "         [[-1.2224e+02, -1.5940e+02, -1.4641e+02,  ..., -1.5233e+02,\n",
      "           -1.5309e+02, -1.5609e+02],\n",
      "          [ 1.0263e+01,  1.7823e+01,  3.8295e+01,  ...,  8.1976e+01,\n",
      "            6.7090e+01,  6.1568e+01],\n",
      "          [-2.5951e+01, -3.7573e+01, -2.4006e+01,  ..., -3.7564e+00,\n",
      "           -7.9989e+00, -6.9785e+00],\n",
      "          ...,\n",
      "          [-2.1250e+01, -3.0665e+01, -1.7564e+01,  ..., -1.5139e+01,\n",
      "           -1.9501e+01, -2.6753e+01],\n",
      "          [-2.1008e+01, -2.8218e+01, -1.6706e+01,  ..., -2.4837e+01,\n",
      "           -2.5742e+01, -3.1106e+01],\n",
      "          [-3.8933e+01, -4.7409e+01, -2.5087e+01,  ..., -2.7937e+01,\n",
      "           -2.5472e+01, -2.4542e+01]],\n",
      "\n",
      "         [[-2.2667e+02, -4.9531e+01, -5.1117e+01,  ..., -5.2010e+01,\n",
      "           -6.6526e+01, -8.2288e+01],\n",
      "          [-2.5110e+02, -3.9659e+01, -3.9530e+01,  ..., -6.9624e+01,\n",
      "           -9.1964e+01, -9.4194e+01],\n",
      "          [-2.4915e+02, -3.4293e+01, -3.2284e+01,  ..., -1.0341e+02,\n",
      "           -7.5455e+01, -1.0261e+02],\n",
      "          ...,\n",
      "          [-2.1685e+02, -2.9723e+01, -2.7873e+01,  ..., -3.5691e+01,\n",
      "           -3.0064e+01, -3.4979e+01],\n",
      "          [-2.1090e+02, -2.5990e+01, -3.8455e+01,  ..., -3.1938e+01,\n",
      "           -3.3976e+01, -3.4955e+01],\n",
      "          [-2.1956e+02, -1.1960e+01, -3.8146e+01,  ..., -2.8979e+01,\n",
      "           -3.0145e+01, -2.8745e+01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.9727e+01, -2.1976e+01, -2.6908e+01,  ..., -2.8386e+00,\n",
      "           -1.4494e+01, -1.1697e+01],\n",
      "          [-3.0340e+01, -5.7781e-01, -1.2083e+00,  ...,  1.2807e+01,\n",
      "            1.2975e+01,  2.2502e+01],\n",
      "          [-3.9737e+01, -4.1497e+00, -1.1430e-01,  ...,  7.1149e+00,\n",
      "            1.0798e+01,  1.7915e+01],\n",
      "          ...,\n",
      "          [-3.4560e+01,  9.1055e-01,  3.7374e+00,  ...,  7.3473e-01,\n",
      "            1.7747e-01,  6.6745e+00],\n",
      "          [-3.6347e+01, -7.6732e-01,  1.9604e+00,  ...,  1.6495e+00,\n",
      "            1.8370e-01,  6.7306e+00],\n",
      "          [-3.4088e+01,  7.3820e+00,  1.4585e+01,  ...,  1.3099e+01,\n",
      "            1.5816e+01,  1.8022e+01]],\n",
      "\n",
      "         [[-3.1770e+01, -2.4487e+00, -8.5895e+00,  ..., -3.2114e+01,\n",
      "           -1.4112e+01, -3.0389e+01],\n",
      "          [-2.6011e+01, -5.8223e+00, -1.8006e+00,  ...,  3.4366e+00,\n",
      "           -2.1854e+01, -1.2438e+01],\n",
      "          [-2.4639e+01, -1.7622e+00,  9.1833e+00,  ..., -1.6686e+01,\n",
      "            1.4174e+01, -1.7031e+01],\n",
      "          ...,\n",
      "          [-2.6636e+01, -5.5556e+00,  1.0194e+01,  ...,  8.0575e+00,\n",
      "            1.3183e+01,  8.5657e+00],\n",
      "          [-2.0726e+01,  4.2468e+00,  7.9974e+00,  ...,  1.1019e+01,\n",
      "            6.1390e+00, -1.5204e+00],\n",
      "          [-1.8236e+01,  1.7682e+01,  1.3090e+01,  ...,  1.3407e+01,\n",
      "            5.8072e+00,  8.7575e+00]],\n",
      "\n",
      "         [[ 5.1244e+01, -5.7302e+01, -1.8471e+01,  ..., -1.9874e+00,\n",
      "           -1.3427e+01, -4.9670e+01],\n",
      "          [-2.5851e+01, -5.5828e+01, -2.7812e+01,  ..., -1.8129e+01,\n",
      "           -7.6204e+00, -2.3068e+01],\n",
      "          [ 3.0009e+00, -2.1655e+01, -2.4234e+00,  ...,  2.8980e+01,\n",
      "           -1.0689e+01,  4.0481e+00],\n",
      "          ...,\n",
      "          [ 3.1387e-01, -2.4807e+01, -2.9218e+00,  ..., -2.2604e+00,\n",
      "           -6.5034e+00, -6.8180e+00],\n",
      "          [-2.0187e+00, -2.6109e+01, -9.8659e-01,  ..., -1.0140e+01,\n",
      "           -5.3469e+00, -4.8377e+00],\n",
      "          [-9.1256e+00, -3.9040e+01, -3.9866e+00,  ..., -3.3571e+00,\n",
      "           -1.1672e+00, -4.7775e+00]]]], device='cuda:0',\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#### input floating number / weight floating number version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 64, out_channels=64, kernel_size = 3, bias = False)\n",
    "weight = model.layer1[0].conv1.weight\n",
    "mean = weight.data.mean()\n",
    "std = weight.data.std()\n",
    "conv_ref.weight = torch.nn.parameter.Parameter(weight.add(-mean).div(std))\n",
    "\n",
    "output_ref = conv_ref(x)\n",
    "print(output_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "significant-whole",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "significant-whole",
    "outputId": "9cb7e839-67b9-4865-e74f-b4c44b2c6d32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0327, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "difference = abs( output_ref - output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c900f",
   "metadata": {},
   "source": [
    "# Resnet20 Quant_aware_training with 2 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f3dbef9",
   "metadata": {
    "id": "joYgZCpmteq0"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def weight_quantization(b):\n",
    "\n",
    "    def uniform_quant(x, b):\n",
    "        xdiv = x.mul((2 ** b - 1))\n",
    "        xhard = xdiv.round().div(2 ** b - 1)\n",
    "        #print('uniform quant bit: ', b)\n",
    "        return xhard\n",
    "\n",
    "    class _pq(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, alpha):\n",
    "            input.div_(alpha)                          # weights are first divided by alpha\n",
    "            input_c = input.clamp(min=-1, max=1)       # then clipped to [-1,1]\n",
    "            sign = input_c.sign()\n",
    "            input_abs = input_c.abs()\n",
    "            input_q = uniform_quant(input_abs, b).mul(sign)\n",
    "            ctx.save_for_backward(input, input_q)\n",
    "            input_q = input_q.mul(alpha)               # rescale to the original range\n",
    "            return input_q\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            grad_input = grad_output.clone()             # grad for weights will not be clipped\n",
    "            input, input_q = ctx.saved_tensors\n",
    "            i = (input.abs()>1.).float()     # >1 means clipped. # output matrix is a form of [True, False, True, ...]\n",
    "            sign = input.sign()              # output matrix is a form of [+1, -1, -1, +1, ...]\n",
    "            #grad_alpha = (grad_output*(sign*i + (input_q-input)*(1-i))).sum()\n",
    "            grad_alpha = (grad_output*(sign*i + (0.0)*(1-i))).sum()\n",
    "            # above line, if i = True,  and sign = +1, \"grad_alpha = grad_output * 1\"\n",
    "            #             if i = False, \"grad_alpha = grad_output * (input_q-input)\"\n",
    "            grad_input = grad_input*(1-i)\n",
    "            return grad_input, grad_alpha\n",
    "\n",
    "    return _pq().apply\n",
    "\n",
    "\n",
    "class weight_quantize_fn(nn.Module):\n",
    "    def __init__(self, w_bit):\n",
    "        super(weight_quantize_fn, self).__init__()\n",
    "        self.w_bit = w_bit-1\n",
    "        self.weight_q = weight_quantization(b=self.w_bit)\n",
    "        self.register_parameter('wgt_alpha', Parameter(torch.tensor(3.0)))\n",
    "\n",
    "    def forward(self, weight):\n",
    "        mean = weight.data.mean()\n",
    "        std = weight.data.std()\n",
    "        weight = weight.add(-mean).div(std)      # weights normalization\n",
    "        weight_q = self.weight_q(weight, self.wgt_alpha)\n",
    "\n",
    "        return weight_q\n",
    "\n",
    "\n",
    "def act_quantization(b):\n",
    "\n",
    "    def uniform_quant(x, b=4):\n",
    "        xdiv = x.mul(2 ** b - 1)\n",
    "        xhard = xdiv.round().div(2 ** b - 1)\n",
    "        return xhard\n",
    "\n",
    "    class _uq(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, input, alpha):\n",
    "            input=input.div(alpha)\n",
    "            input_c = input.clamp(max=1)  # Mingu edited for Alexnet\n",
    "            input_q = uniform_quant(input_c, b)\n",
    "            ctx.save_for_backward(input, input_q)\n",
    "            input_q = input_q.mul(alpha)\n",
    "            return input_q\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            grad_input = grad_output.clone()\n",
    "            input, input_q = ctx.saved_tensors\n",
    "            i = (input > 1.).float()\n",
    "            #grad_alpha = (grad_output * (i + (input_q - input) * (1 - i))).sum()\n",
    "            grad_alpha = (grad_output * (i + (0.0)*(1-i))).sum()\n",
    "            grad_input = grad_input*(1-i)\n",
    "            return grad_input, grad_alpha\n",
    "\n",
    "    return _uq().apply\n",
    "\n",
    "\n",
    "class QuantConv2d(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=False):\n",
    "        super(QuantConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups,\n",
    "                                          bias)\n",
    "        self.layer_type = 'QuantConv2d'\n",
    "        self.bit = 2\n",
    "        self.weight_quant = weight_quantize_fn(w_bit=self.bit)\n",
    "        self.act_alq = act_quantization(self.bit)\n",
    "        self.act_alpha = torch.nn.Parameter(torch.tensor(8.0))\n",
    "        self.weight_q  = torch.nn.Parameter(torch.zeros([out_channels, in_channels, kernel_size, kernel_size]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight_q = self.weight_quant(self.weight)\n",
    "        #self.register_parameter('weight_q', Parameter(weight_q))  # Mingu added\n",
    "        self.weight_q = torch.nn.Parameter(weight_q)  # Store weight_q during the training\n",
    "        x = self.act_alq(x, self.act_alpha)\n",
    "        return F.conv2d(x, weight_q, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def show_params(self):\n",
    "        wgt_alpha = round(self.weight_quant.wgt_alpha.data.item(), 3)\n",
    "        act_alpha = round(self.act_alpha.data.item(), 3)\n",
    "        print('clipping threshold weight alpha: {:2f}, activation alpha: {:2f}'.format(wgt_alpha, act_alpha))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d840f6a0",
   "metadata": {
    "id": "OifnJ9ndtlfc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "resnet for cifar in pytorch\n",
    "Reference:\n",
    "[1] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.\n",
    "[2] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, 2016.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \" 3x3 convolution with padding \"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "def Quantconv3x3(in_planes, out_planes, stride=1):\n",
    "    \" 3x3 quantized convolution with padding \"\n",
    "    return QuantConv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion=1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, float=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if float:\n",
    "            self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "            self.conv2 = conv3x3(planes, planes)\n",
    "        else:\n",
    "            self.conv1 = Quantconv3x3(inplanes, planes, stride)\n",
    "            self.conv2 = Quantconv3x3(planes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion=4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes*4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes*4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet_Cifar(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=10, float=False):\n",
    "        super(ResNet_Cifar, self).__init__()\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = QuantConv2d(3, 16, kernel_size=3, stride=1)# nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0], float=float)\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2, float=float)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2, float=float)\n",
    "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
    "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, float=False):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                QuantConv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False)\n",
    "                if float is False else nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1,\n",
    "                                                 stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, float=float))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, float=float))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def show_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, QuantConv2d):\n",
    "                m.show_params()\n",
    "\n",
    "\n",
    "def resnet20_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [3, 3, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet32_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [5, 5, 5], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet44_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [7, 7, 7], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet56_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [9, 9, 9], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet110_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [18, 18, 18], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet1202_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [200, 200, 200], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet164_quant(**kwargs):\n",
    "    model = ResNet_Cifar(Bottleneck, [18, 18, 18], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet1001_quant(**kwargs):\n",
    "    model = ResNet_Cifar(Bottleneck, [111, 111, 111], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "    # net = resnet20_cifar(float=True)\n",
    "    # y = net(torch.randn(1, 3, 64, 64))\n",
    "    # print(net)\n",
    "    # print(y.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fcdf8e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "radical-fifty",
    "outputId": "9afb2858-e719-496f-b21e-a7f31f5a4754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "ResNet_Cifar(\n",
      "  (conv1): QuantConv2d(\n",
      "    3, 16, kernel_size=(3, 3), stride=(1, 1), bias=False\n",
      "    (weight_quant): weight_quantize_fn()\n",
      "  )\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "model_name = \"resnet20_quant_2bit\"\n",
    "model = resnet20_quant()\n",
    "\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "# def adjust_learning_rate(optimizer, epoch):\n",
    "#     \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "#     adjust_list = [10, 80, 120]\n",
    "#     if epoch in adjust_list:\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = param_group['lr'] * 0.1\n",
    "\n",
    "# Updated learning rate adjustment with warm-up and cosine annealing\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    if epoch < 5:  # Warm-up for the first 5 epochs\n",
    "        lr = 0.001 + (0.1 - 0.001) * epoch / 5\n",
    "    else:  # Cosine Annealing after warm-up\n",
    "        lr_min = 0.001\n",
    "        lr_max = 0.1\n",
    "        lr = lr_min + (lr_max - lr_min) * (1 + math.cos(math.pi * (epoch - 5) / (220 - 5))) / 2  # Adjust for 220 epochs\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70ee8589",
   "metadata": {
    "id": "junior-reminder"
   },
   "outputs": [],
   "source": [
    "# model.cuda()\n",
    "\n",
    "# lr = 0.005\n",
    "# weight_decay = 5e-5\n",
    "# epochs = 50\n",
    "# best_prec = 0\n",
    "# def adjust_learning_rate(optimizer, epoch):\n",
    "#     if epoch < 10:  # Warm-up for the first 5 epochs\n",
    "#         lr = 0.001 + (0.1 - 0.001) * epoch / 5\n",
    "#     else:  # Cosine Annealing after warm-up\n",
    "#         lr_min = 0.001\n",
    "#         lr_max = 0.1\n",
    "#         lr = lr_min + (lr_max - lr_min) * (1 + math.cos(math.pi * (epoch - 5) / (220 - 5))) / 2  # Adjust for 220 epochs\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         param_group['lr'] = lr\n",
    "# #model = nn.DataParallel(model).cuda()\n",
    "\n",
    "# model.train()\n",
    "# criterion = nn.CrossEntropyLoss().cuda()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "# #cudnn.benchmark = True\n",
    "\n",
    "# if not os.path.exists('result'):\n",
    "#     os.makedirs('result')\n",
    "# fdir = 'result/'+str(model_name)\n",
    "# if not os.path.exists(fdir):\n",
    "#     os.makedirs(fdir)\n",
    "\n",
    "\n",
    "# for epoch in range(0, epochs):\n",
    "#     adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "#     train(trainloader, model, criterion, optimizer, epoch)\n",
    "\n",
    "#     # evaluate on test set\n",
    "#     print(\"Validation starts\")\n",
    "#     prec = validate(testloader, model, criterion)\n",
    "\n",
    "#     # remember best precision and save checkpoint\n",
    "#     is_best = prec > best_prec\n",
    "#     best_prec = max(prec,best_prec)\n",
    "#     print('best acc: {:1f}'.format(best_prec))\n",
    "#     save_checkpoint({\n",
    "#         'epoch': epoch + 1,\n",
    "#         'state_dict': model.state_dict(),\n",
    "#         'best_prec': best_prec,\n",
    "#         'optimizer': optimizer.state_dict(),\n",
    "#     }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d1f4c53",
   "metadata": {
    "id": "decreased-harris"
   },
   "outputs": [],
   "source": [
    "# HW\n",
    "\n",
    "#  1. Train with 4 bits for both weight and activation to achieve >90% accuracy\n",
    "#  2. Find x_int and w_int for the 2nd convolution layer\n",
    "#  3. Check the recovered psum has similar value to the un-quantized original psum\n",
    "#     (such as example 1 in W3S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a162164",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "entertaining-queensland",
    "outputId": "985359e8-7129-460e-bb0e-5d8b62fdd565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 7379/10000 (74%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/resnet20_quant_2bit/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
