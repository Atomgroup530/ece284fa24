{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vaXRgN3XqCzX",
   "metadata": {
    "id": "vaXRgN3XqCzX"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1, bias=False),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def show_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.show_params()\n",
    "\n",
    "\n",
    "def VGG11():\n",
    "    return VGG('VGG11')\n",
    "\n",
    "\n",
    "def VGG13():\n",
    "    return VGG('VGG13')\n",
    "\n",
    "\n",
    "#def VGG16():\n",
    "#    return VGG('VGG16')\n",
    "\n",
    "def VGG16(**kwargs):\n",
    "    model = VGG(vgg_name = 'VGG16', **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def VGG19():\n",
    "    return VGG('VGG19')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "radical-fifty",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "radical-fifty",
    "outputId": "dd6a1d88-0fe9-4d89-f5eb-624cd9752624"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()   ## at the begining of each epoch, this should be reset\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()  # measure current time\n",
    "\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)  # data loading time\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end) # time spent to process one batch\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True) # topk(k, dim=None, largest=True, sorted=True)\n",
    "                                               # will output (max value, its index)\n",
    "    pred = pred.t()           # transpose\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))   # \"-1\": calculate automatically\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)  # view(-1): make a flattened 1D tensor\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))   # correct: size of [maxk, batch_size]\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n    ## n is impact factor\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Start with lr = 0.1, reduce by 10x at 15, 25, and 35 epochs\"\"\"\n",
    "    adjust_list = [15, 25, 35, 50, 70]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MAt3inhZSs-e",
   "metadata": {
    "id": "MAt3inhZSs-e"
   },
   "source": [
    "# VGG16 with CrossEntropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tCL4aLYbJ85C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCL4aLYbJ85C",
    "outputId": "88929b2e-9064-449d-9c65-a0a5ad86eb2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-6aa4d58f6b72>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fdir)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/40]\tTime 0.148 (0.148)\tLoss 0.3038 (0.3038)\tPrec 91.406% (91.406%)\n",
      " * Prec 90.470% \n",
      "first conv layer weights absolute sum: 225.03768920898438\n"
     ]
    }
   ],
   "source": [
    "model_name = \"VGG16\"\n",
    "model = VGG16()\n",
    "fdir = 'result/'+str(model_name)+'/model_best.pth.tar'\n",
    "\n",
    "checkpoint = torch.load(fdir)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "prec = validate(testloader, model, criterion)\n",
    "print(f'first conv layer weights absolute sum: {model.features[0].weight.abs().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7yrDnMKTAXH",
   "metadata": {
    "id": "q7yrDnMKTAXH"
   },
   "source": [
    "# VGG16 with Even Weight CrossEntropy and Weights' Absolute Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fBcXfVo63A4n",
   "metadata": {
    "id": "fBcXfVo63A4n"
   },
   "outputs": [],
   "source": [
    "model_name = \"VGG16_loss0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OP7hPH2W2JB7",
   "metadata": {
    "id": "OP7hPH2W2JB7"
   },
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "lr = 0.01  # Start at 0.1\n",
    "n_epochs = 10\n",
    "best_prec = 0\n",
    "weight_decay = 1e-4\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.95, weight_decay=weight_decay)\n",
    "model.train()  # prep model for training\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "\n",
    "fdir = 'result/' + str(model_name)\n",
    "\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_loss1 = 0.0\n",
    "    train_loss2 = 0.0\n",
    "\n",
    "    for data, target in trainloader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss1 = criterion(output, target)\n",
    "        loss2 = model.features[0].weight.abs().sum()\n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate losses\n",
    "        train_loss1 += loss1.item() * data.size(0)\n",
    "        train_loss2 += loss2.item() * data.size(0)\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        # Calculate average losses\n",
    "        train_loss1 /= len(trainloader.dataset)\n",
    "        train_loss2 /= len(trainloader.dataset)\n",
    "        train_loss /= len(trainloader.dataset)\n",
    "\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    print('Training Loss1: {:.6f}'.format(epoch+1, train_loss1))\n",
    "    print('Training Loss2: {:.6f}'.format(epoch+1, train_loss2))\n",
    "    print('Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    prec = validate(testloader, model, criterion)\n",
    "    # Remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec, best_prec)\n",
    "    print('Best accuracy: {:.3f}'.format(best_prec))\n",
    "\n",
    "    # Save model checkpoint\n",
    "    save_checkpoint({\n",
    "            'epoch': epoch+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': train_loss,\n",
    "            'best_prec': best_prec,\n",
    "            }, is_best, fdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sorted-niger",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sorted-niger",
    "outputId": "55fa35a0-d318-469d-f62d-11456a3d3dcf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-6d7d4bb3295b>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fdir)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 7132/10000 (71%)\n",
      "\n",
      "first conv layer weights absolute sum: 33.18050003051758\n"
     ]
    }
   ],
   "source": [
    "fdir = 'result/'+str(model_name)+'/model_best.pth.tar'\n",
    "checkpoint = torch.load(fdir)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "train_loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.cuda(), target.cuda() # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))\n",
    "print(f'first conv layer weights absolute sum: {model.features[0].weight.abs().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ENX73C5kTN1D",
   "metadata": {
    "id": "ENX73C5kTN1D"
   },
   "source": [
    "# VGG16 with CrossEntropy Loss and weighted Weights' Absolute Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "blond-builder",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "blond-builder",
    "outputId": "285be747-ebc0-4837-f548-47b3ca96ac78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-d20061016a2f>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fdir)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"VGG16\"\n",
    "model = VGG16()\n",
    "fdir = 'result/'+str(model_name)+'/model_best.pth.tar'\n",
    "\n",
    "checkpoint = torch.load(fdir)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "model.cuda();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pF7AJyZLUYMV",
   "metadata": {
    "id": "pF7AJyZLUYMV"
   },
   "outputs": [],
   "source": [
    "model_name = \"VGG16_loss1\"\n",
    "# number of epochs to train the model\n",
    "lr = 0.01  # Start at 0.1\n",
    "n_epochs = 50\n",
    "best_prec = 0\n",
    "weight_decay = 1e-4\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.95, weight_decay=weight_decay)\n",
    "model.train()  # prep model for training\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "\n",
    "fdir = 'result/' + str(model_name)\n",
    "\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train_loss = 0.0\n",
    "    train_loss1 = 0.0\n",
    "    train_loss2 = 0.0\n",
    "\n",
    "    for data, target in trainloader:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss1 = criterion(output, target)\n",
    "        loss2 = model.features[0].weight.abs().sum()\n",
    "        loss = loss1 + 0.05*loss2\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate losses\n",
    "        train_loss1 += loss1.item() * data.size(0)\n",
    "        train_loss2 += loss2.item() * data.size(0)\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        # Calculate average losses\n",
    "        train_loss1 /= len(trainloader.dataset)\n",
    "        train_loss2 /= len(trainloader.dataset)\n",
    "        train_loss /= len(trainloader.dataset)\n",
    "\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    print('Training Loss1: {:.6f}'.format(epoch+1, train_loss1))\n",
    "    print('Training Loss2: {:.6f}'.format(epoch+1, train_loss2))\n",
    "    print('Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    prec = validate(testloader, model, criterion)\n",
    "    # Remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec, best_prec)\n",
    "    print('Best accuracy: {:.3f}'.format(best_prec))\n",
    "\n",
    "    # Save model checkpoint\n",
    "    save_checkpoint({\n",
    "            'epoch': epoch+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': train_loss,\n",
    "            'best_prec': best_prec,\n",
    "            }, is_best, fdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "jx2zutqBU19u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jx2zutqBU19u",
    "outputId": "df4c5942-8a20-4859-aa31-02843b4d7bdd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-6d7d4bb3295b>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fdir)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 7414/10000 (74%)\n",
      "\n",
      "first conv layer weights absolute sum: 0.655273973941803\n"
     ]
    }
   ],
   "source": [
    "fdir = 'result/'+str(model_name)+'/model_best.pth.tar'\n",
    "checkpoint = torch.load(fdir)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "train_loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.cuda(), target.cuda() # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))\n",
    "print(f'first conv layer weights absolute sum: {model.features[0].weight.abs().sum()}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
